\documentclass[12pt]{article}

\usepackage{ishn}

\makeindex[intoc]

\begin{document}

\hypersetup{pageanchor=false}
\begin{titlepage}
	\begin{center}
		\vspace*{1em}
		\Huge
		\textbf{III Advanced Probability}

		\vspace{1em}
		\large
		Ishan Nath, Michaelmas 2024

		\vspace{1.5em}

		\Large

		Based on Lectures by Prof. Jason Miller

		\vspace{1em}

		\large
		\today
	\end{center}
	
\end{titlepage}
\hypersetup{pageanchor=true}

\tableofcontents

\newpage

% lecture 1

\setcounter{section}{-1}

\section{Introduction}%
\label{sec:intro}

We will be following the lecture notes by Perla Sousi.

This course is divided into seven main topics:
\begin{enumerate}[label=Chapter \arabic*.]
	\item Conditional expectation. We have seen how to define $\mathbb{E}[Y | X = x]$ for $X$ a discrete-valued random variable, or one with continuous density wrt. the Lebesgue measure. We will define more generally $\mathbb{E}[Y|\mathcal{G}]$, for $\mathcal{G}$ a $\sigma$-algebra.
	\item Discrete-time martingales. A martingale is a sequence $(X_n)$ of integrable random variables such that $\mathbb{E}[X_{n+1} \mid X_1, \ldots, X_n] = X_n$, for all $n$. A basic example is a simple symmetric random walk. We will be looking at properties, bounds and computations.
	\item Continuous time processes. Examples are
		\begin{itemize}
			\item Martingales in continuous time.
			\item Some continuous continuous-time processes.
		\end{itemize}
	\item Weak convergence. This is a notion of convergence which extends convergence in distribution, and is related to other notions of convergence.
	\item Large deviations. Here we estimate how unlikely rare events are. An example if the sample mean for $(\xi_j)$ iid, with mean $\mu$, which converges to $\mu$ by SLLN. We can ask what the probability is that the sample mean deviates from $\mu$ by $\eps$, which can be answered by Cramer's theorem.
	\item Brownian motion. This is a fundamental object, which is a continuous time stochastic process. We will look at the definition, Markov processes, and its relationship with PDEs.
	\item Poisson random measures. This is the generalization of the standard Poisson process on $\mathbb{R}_+$.
\end{enumerate}

\newpage

\section{Conditional Expectation}%
\label{sec:cond_e}

Recall that a probability space $(\Omega, \mathcal{F}, \mathbb{P})$ consists of a set $\Omega$, a $\sigma$-algebra on $\Omega$, meaning:
\begin{itemize}
	\item $\Omega \in \mathcal{F}$,
	\item $A \in \mathcal{F} \implies A^{c} \in \mathcal{F}$,
	\item if $(A_n)$ is in $\mathcal{F}$, then $\bigcup A_n \in \mathcal{F}$,
\end{itemize}
and $\mathbb{P}$ is a probability measure on $(\Omega, \mathcal{F})$, meaning:
\begin{itemize}
	\item $\mathbb{P} : \mathcal{F} \to [0, 1]$ such that
	\item $\mathbb{P}(\Omega) = 1$, $\mathbb{P}(\emptyset) = 0$,
	\item if $(A_n)$ are pairwise disjoint in $\mathcal{F}$, then $\mathbb{P}(\bigcup A_n) = \sum \mathbb{P}(A_n)$.
\end{itemize}
An important $\sigma$-algebra is the Borel $\sigma$-algebra $\mathcal{B}$, which for $\mathbb{R}$ is the intersection of all $\sigma$-algebra on $\mathbb{R}$ which contain the open sets in $\mathbb{R}$.

Recall that $X : \Omega \to \mathbb{R}$ is a random variable if $X^{-1}(U) \in \mathcal{F}$ for all $U \subseteq \mathbb{R}^n$ open. This is equivalent to $X^{-1}(B) \in \mathcal{F}$ for all $B \in \mathcal{B}$.

\begin{definition}
	Suppose that $\mathcal{A}$ is a collection of sets in $\Omega$. Then
	\[
		\sigma(\mathcal{A}) = \bigcap \{ \mathcal{E} \mid \mathcal{E} \text{ is a $\sigma$-algebra containing } \mathcal{A}\}.
	\]
\end{definition}

If $(X_i)$ is a collection of random variables, then
\[
	\sigma(X_i \mid i \in I) = \sigma(\{ \omega \in \Omega \mid X_i(\omega) \in B\}, i \in I, B \in \mathcal{B}).
\]
This is the smallest $\sigma$-algebra which makes $X_i$ measurable for all $i \in I$.

Let $A \in \mathcal{F}$. Set
\[
\mathbbm{1}_{A}(x) = \mathbbm{1}(x \in A) = 
\begin{cases}
	1 & x \in A, \\
	0 & \text{else}.
\end{cases}
\]
We can define expectation as follows:
\begin{itemize}
	\item For $C_i \geq 0$, $A_i \in \mathcal{F}$, set
		\[
		\mathbb{E}\left[\sum_{i = 1}^n C_i \mathbbm{1}_{A_i} \right] = \sum_{i = 1}^n C_i \mathbb{P}(A_i).
		\]
	\item For $X \geq 0$, set $X_n = (2^{-n} \lfloor 2^n X \rfloor) \wedge n$, so that $X_n \uparrow X$ as $n \to \infty$. Set
		\[
		\mathbb{E}[X] = \lim_{n \to \infty} \mathbb{E}[X_n].
		\]
	\item If $X$ is a general random variable, write $X = X^+ - X^-$, where $X^+ = \max(X, 0)$, $X^- = \max(-X, 0)$. If $\mathbb{E}[X^+]$ or $\mathbb{E}[X^-]$ is finite, set
		\[
		\mathbb{E}[X] = \mathbb{E}[X^+] - \mathbb{E}[X^-].
		\]	
\end{itemize}
Call $X$ integrable if $\mathbb{E}[|X|] < \infty$.

Recall that if $A, B \in \mathcal{F}$ with $\mathbb{P}(B) > 0$, then
\[
\mathbb{P}(A \mid B) = \mathbb{P}(A \cap B)/\mathbb{P}(B).
\]
If $X$ is integrable and $A \in \mathcal{F}$ with $\mathbb{P}(A) > 0$, then
\[
\mathbb{E}[X | A] = \mathbb{E}[X \mathbbm{1}_{A}] / \mathbb{P}(A).
\]
Our goal is to extend this definition to
\[
\mathbb{E}[X | \mathcal{G}],
\]
where $\mathcal{G}$ is a $\sigma$-algebra in $\mathcal{F}$. The main idea is that $\mathbb{E}[X | \mathcal{G}]$ is the best prediction of $X$ given $\mathcal{G}$.

Hence it should be a $\mathcal{G}$-measurable random variable $Y$, which minimizes
\[
\mathbb{E}[(X - Y)^2].
\]

% lecture 2

\subsection{Discrete Case}%
\label{sub:discrete_cond}


As a warm-up, we consider the discrete case.

Suppose that $X$ is integrable, $(B_i)$ is countable and disjoint with $\Omega = \bigcup B_i$, and $\mathcal{G} = (B_i, i \in I)$. Set $X' = \mathbb{E}[X | \mathcal{G}]$ by
\[
X' = \sum_{i \in I} \mathbb{E}[X | B_i ] \mathbbm{1}_{B_i},
\]
with the convention that $\mathbb{E}[X | B_i] = 0$ if $\mathbb{P}(B_i) = 0$. If $\omega \in \Omega$, then
\[
X'(\omega) = \sum_i \mathbb{E}[X | B_i] \mathbbm{1}(\omega \in B_i).
\]
We look at some important properties of $X'$.
\begin{enumerate}
	\item $X'$ is $\mathcal{G}$-measurable, as it is a linear combination of the $\mathcal{G}$-measurable random variables $\mathbbm{1}_{B_i}$.
	\item $X'$ is integrable, as
		\[
		\mathbb{E}[|X'|] \leq \sum_{i \in I} \mathbb{E}[|X| \mathbbm{1}_{B_i}] = \mathbb{E}[|X|] < \infty.
		\]
	\item If $G \in \mathcal{G}$, then
		\[
		\mathbb{E}[X \mathbbm{1}_{G}] = \mathbb{E}[X' \mathbbm{1}_{G}].
		\]
\end{enumerate}

\subsection{Existence and Uniqueness}%
\label{sub:e_u}

We need a couple of important facts from measure theory:

\begin{theorem}[Monotone Convergence Theorem]
	If $(X_n)$ is a sequence of random variables with $X_n \geq 0$ for all $n$ and $X_n \uparrow X$ almost-surely, then $\mathbb{E}[X_n] \uparrow \mathbb{E}[X]$.
\end{theorem}

\begin{theorem}[Dominated Convergence Theorem]
	Suppose that $(X_n)$ is a sequence of random variables with $X_n \to X$ almost-surely and $|X_n| \leq Y$ for all $n$ where $Y$ is an integrable random variable, then $\mathbb{E}[X_n] \to \mathbb{E}[X]$ as $n \to \infty$.
\end{theorem}

The construction of $\mathbb{E}[X | \mathcal{G}]$ requires a few steps; in the case that $X \in L^2$, then this can be defined by orthogonal projection.

We need to recall a few things about $L^p$ spaces. Suppose that $p \in [1, \infty)$, and $X$ is a random variables in $(\Omega, \mathcal{F})$. Then,
\[
\|X\|_p = \mathbb{E}[|X|^p]^{1/p}.
\]
Now
\[
	L^p = L^p(\Omega, \mathcal{F}, \mathbb{P}) = \{X \text{ with } \|X\|_p < \infty\}.
\]
For $p = \infty$,
\[
	\|X\|_\infty = \inf\{ \lambda \mid |X| \leq \lambda \text{ almost surely}\},
\]
and
\[
	L^\infty = L^\infty(\Omega, \mathcal{F}, \mathbb{P}) = \{X \text{ with } \|X\|_\infty < \infty\}.
\]
Two random variables in $L^p$ are equivalent if they agree almost-surely. Then $\mathcal{L}^p$ is the set of equivalence classes under this equivalence relation.

\begin{theorem}
	The space $(\mathcal{L}^2, \|\cdot\|_2)$ is a Hilbert space with inner product
	\[
	\langle X, Y \rangle = \mathbb{E}[XY].
	\]
	If $\mathcal{H}$ is a closed subspace of $\mathcal{L}^2$, then for all $X \in \mathcal{L}^2$, there exists $Y \in \mathcal{H}$ such that
	\[
	\|X - Y\|_2 = \inf_{Z \in \mathcal{H}}\|X - Z\|_2,
	\]
	and  $\langle X - Y, Z \rangle = 0$ for all $Z \in \mathcal{H}$. $Y$ is the \emph{orthogonal projection}\index{orthogonal projection} of $X$ onto $\mathcal{H}$.
\end{theorem}

\begin{theorem}
	Let $X$ be an integrable random variable, and $\mathcal{G} \subseteq \mathcal{F}$ a $\sigma$-algebra. Then there exists a random variable $Y$ such that:
	\begin{enumerate}[\normalfont(i)]
		\item $Y$ is $\mathcal{G}$-measurable.
		\item $Y$ is integrable, with
			\[
			\mathbb{E}[X \mathbbm{1}_{A}] = \mathbb{E}[Y \mathbbm{1}_{A}]
			\]
			for all $A \in \mathcal{G}$.
	\end{enumerate}
	If $Y'$ satisfies these two properties, then $Y = Y'$ almost surely. $Y$ is called (a version of) the \emph{conditional expectation}\index{conditional expectation} of $X$ given $\mathcal{G}$, and we write
	\[
	Y = \mathbb{E}[X | \mathcal{G}].
	\]
	If $\mathcal{G} = \sigma(G)$ for a random variable $G$, we write $Y = \mathbb{E}[X | G]$.
\end{theorem}

\begin{proofbox}
	We begin by proving uniqueness. Suppose $Y, Y'$ are two conditional expectation. Then
	\[
		A = \{Y > Y'\} \in \mathcal{G},
	\]
	so by (ii),
	\[
	\mathbb{E}[(Y - Y') \mathbbm{1}_{A}] = \mathbb{E}[X \mathbbm{1}_{A}] - \mathbb{E}[X \mathbbm{1}_{A}] = 0,
	\]
	but the LHS is non-negative, so the only way this can hold is if $Y \leq Y'$ almost surely. Similarly, $Y \geq Y'$ almost surely, so $Y = Y'$ almost surely.

	Now we are ready to tackle existence. The first step is by restricting to $\mathcal{L}^2$ function.

	Take $X \in \mathcal{L}^2$. Note that $\mathcal{L}^2(\Omega, \mathcal{G}, \mathbb{P})$ is a closed subspace of $\mathcal{L}^2(\Omega, \mathcal{F}, \mathbb{P})$.

	Set $Y = \mathbb{E}[X | \mathcal{G}]$ to be the orthogonal projections of $X$ onto $\mathcal{L}^2(\Omega, \mathcal{G}, \mathbb{P})$.

	It is automatically true that (i) holds. Suppose $A \in \mathcal{G}$. Then
	\[
	\mathbb{E}[(X - Y) \mathbbm{1}_{A}] = 0,
	\]
	by orthogonality, since $\mathbbm{1}_{A}$ is $\mathcal{G}$-measurable. So
	\[
	\mathbb{E}[X \mathbbm{1}_{A}] = \mathbb{E}[Y \mathbbm{1}_{A}],
	\]
	so (ii) holds. Finally $Y$ is integrable as it is specified to be in $\mathcal{L}^2$.


	As an aside, notice if $X \geq 0$, then $Y = \mathbb{E}[X | \mathcal{G}] \geq 0$ almost-surely. Since $\{Y < 0\} \in \mathcal{G}$ and
	\[
		\mathbb{E}[Y \mathbbm{1}_{\{Y < 0\}}] = \mathbb{E}[X \mathbbm{1}_{\{Y < 0\}}] \geq 0,
	\]
	which can only happen if $\mathbb{P}(Y < 0) = 0$.

	Our second step is to prove this for $X \geq 0$. In this case, let $X_n = X \wedge n$. Then $X_n \in \mathcal{L}^2$ for all $n$, so there exists $\mathcal{G}$-measurable random variables $Y_n$ so that
	\[
	\mathbb{E}[Y_n \mathbbm{1}_{A}] = \mathbb{E}[(X \wedge n) \mathbbm{1}_{A}],
	\]
	for all $A \in \mathcal{G}$. Now $(X_n)$ is increasing in $n$, so $(Y_n)$ are also increasing by the above aside. Set
	\[
	Y = \limsup_{n} Y_n.
	\]
	We now have to check the definitions. Note $Y$ is $\mathcal{G}$-measurable as it is a limsup of $\mathcal{G}$-measurable functions. So we check the second definition.

	For $A \in \mathcal{G}$,
	\[
		\mathbb{E}[Y \mathbbm{1}_{A}] \overset{MCT}= \lim_n \mathbb{E}[Y_n \mathbbm{1}_{A}] = \lim_n \mathbb{E}[(X \wedge n) \mathbbm{1}_{A}] \overset{MCT}= \mathbb{E}[X \mathbbm{1}_{A}].
	\]
	We did not check integrability, but notice that setting $A = G$, $\mathbb{E}[Y] = \mathbb{E}[X]$, and $X$ is non-negative, hence so is $Y$. Thus $Y$ is integrable.

	Finally we prove the result for $X \in \mathcal{L}^1$. We can apply step 2 to $X^+$ and $X^-$, and so can set
	\[
	\mathbb{E}[X|\mathcal{G}] = \mathbb{E}[X^+|\mathcal{G}] - \mathbb{E}[X^-|\mathcal{G}].
	\]
	This is $\mathcal{G}$-measurable and integrable as it is the difference of two $\mathcal{G}$-measurable and integrable random variables, and satisfies (ii) since both of the random variables satisfy (ii).
\end{proofbox}

% lecture 3

\begin{remark}
	\begin{itemize}
		\item[]
		\item The proof also works for $X \geq 0$, and not necessarily integrable. Then $\mathbb{E}[X | \mathcal{G}]$ is not necessarily integrable.
		\item The property
			\[
			\mathbb{E}[\mathbb{E}[X|\mathcal{G}]\mathbbm{1}_{A}] = \mathbb{E}[X \mathbbm{1}_{A}]
			\]
			for all $A \in \mathcal{G}$, is equivalent to
			\[
			\mathbb{E}[\mathbb{E}[X|\mathcal{G}]Y] = \mathbb{E}[XY],
			\]
			for all $Y$ bounded and $\mathcal{G}$-measurable.
	\end{itemize}
\end{remark}

\subsection{Properties of Expectation}%
\label{sub:eepy}

\begin{definition}
	A collection of $\sigma$-algebras $(\mathcal{G}_i)$ in $\mathcal{G}$ is \emph{independent}\index{independent} if whenever $G_i \in \mathcal{G}_i$ and $i_1, \ldots, i_n$ distinct, then
	\[
	\mathbb{P}(G_{i_1} \cap \cdots \cap G_{i_n}) = \prod_{k = 1}^n \mathbb{P}(G_{ik}).
	\]
	We say that a random variable $X$ is \emph{independent} of a $\sigma$-algebra $\mathcal{G}$ if $\sigma(X)$ is independent of $\mathcal{G}$.
\end{definition}

\begin{proposition}
	Let $X, Y \in \mathcal{L}^2$, and $\mathcal{G} \subseteq \mathcal{F}$ a $\sigma$-algebra. Then:
	\begin{enumerate}[\normalfont(i)]
		\item $\mathbb{E}[\mathbb{E}[X|\mathcal{G}]] = \mathbb{E}[X]$.
		\item If $X$ is $\mathcal{G}$-measurable, then $\mathbb{E}[X|\mathcal{G}] = X$ almost surely.
		\item If $X$ is independent of $\mathcal{G}$, then $\mathbb{E}[X|\mathcal{G}] = \mathbb{E}[X]$.
		\item If $X \geq 0$ almost surely, then $\mathbb{E}[X|\mathcal{G}] \geq 0$ almost surely.
		\item For any $\alpha, \beta \in \mathbb{R}$, $\mathbb{E}[\alpha X + \beta Y \mid \mathcal{G}] = \alpha \mathbb{E}[X|\mathcal{G}] + \beta \mathbb{E}[Y|\mathcal{G}]$.
		\item $|\mathbb{E}[X|\mathcal{G}]| \leq \mathbb{E}[|X||\mathcal{G}]$ almost-surely.
	\end{enumerate}
\end{proposition}

\begin{theorem}[Fatou's Lemma]
	If $(X_n)$ is a sequence of random variables with $X_n \geq 0$, then
	\[
	\mathbb{E}[\liminf_n X_n] \leq \liminf_n \mathbb{E}[X_n].
	\]
\end{theorem}

\begin{theorem}[Jensen's Inequality]
	Let $X \in \mathcal{L}^2$ and let $\phi : \mathbb{R} \to (-\infty, \infty]$ be a convex function. Then,
	\[
	\mathbb{E}[\phi(X)] \geq \phi(\mathbb{E}[X]).
	\]
\end{theorem}

\begin{proposition}
	Let $\mathcal{G} \subseteq \mathcal{F}$ be a $\sigma$-algebra.
	\begin{enumerate}[\normalfont1.]
		\item If $(X_n)$ is an increasing sequence of random variables with $X_n \geq 0$ for all $n$ and $X_n \uparrow X$, then
			\[
			\mathbb{E}[X_n | \mathcal{G}] \uparrow \mathbb{E}[X |\mathcal{G}],
			\]
			almost-surely (conditional MCT).
		\item If $X_n \geq 0$, then
			\[
			\mathbb{E}[\liminf_n X_n | \mathcal{G}] \leq \liminf_n \mathbb{E}[X_n | \mathcal{G}]
			\]
			(conditional Fatou's lemma).
		\item If $X_n \to X$ and $|X_n| \leq Y$ almost-surely for all $n$ and $Y \in \mathcal{L}^1$, then
			\[
			\lim_{n \to \infty} \mathbb{E}[X_n | \mathcal{G}] = \mathbb{E}[X | \mathcal{G}],
			\]
			almost-surely (conditional DCT).
		\item If $X \in \mathcal{L^1}$ and $\phi : \mathbb{R} \to (-\infty, \infty]$ is convex such that either $\phi(X) \in \mathcal{L}^1$ or $\phi(X) \geq 0$, then:
			\[
			\mathbb{E}[\phi(X) | \mathcal{G}] \geq \phi(\mathbb{E}[X | \mathcal{G}])
			\]
			almost surely (convex Jensen's). In particular, for all $1 \leq p < \infty$,
			\[
			\|\mathbb{E}[X|\mathcal{G}]\|_p \leq \|X\|_p.
			\]
	\end{enumerate}
\end{proposition}

\begin{proofbox}


	1. Let $Y_n$ be a version of of $\mathbb{E}[X_n | \mathcal{G}]$. Since $0 \leq X_n \uparrow X$ as $n \to \infty$, we have that $Y_n \geq 0$ and are increasing.

	Define $Y = \limsup_{n \to \infty} Y_n$. We will show that $ = \mathbb{E}[X|\mathcal{G}]$.
	\begin{itemize}
		\item $Y$ is $\mathcal{G}$-measurable as it is a $\limsup$ of $\mathcal{G}$-measurable random variables.
		\item For $A \in \mathcal{G}$,
			\[
				\mathbb{E}[ X \mathbbm{1}_{A}] \overset{MCT}= \lim_{n \to \infty} \mathbb{E}[X_n \mathbbm{1}_{A}] = \lim_{n \to \infty} \mathbb{E}[Y_n \mathbbm{1}_{A}] \overset{MCT}= \mathbb{E}[Y \mathbbm{1}_{A}].
			\]
	\end{itemize}
	2. The sequence $\inf_{k \geq n} X_k$ is increasing in $n$. Moreover,
	\[
	\lim_{n \to \infty} \inf_{k \geq n} X_k = \liminf_{n \to \infty} X_n.
	\]
	By 1,
	\[
	\lim_{n \to \infty} \mathbb{E}[ \inf_{ k \geq n} X_k | \mathcal{G}] = \mathbb{E}[ \liminf_{n \to \infty} X_n | \mathcal{G}].
	\]
	But also,
	\[
	\mathbb{E}[\inf_{k \geq n} X_k |\mathcal{G}] \leq \inf_{k \geq n} \mathbb{E}[X_k | \mathcal{G}]
	\]
	almost-surely, by monotonicity. Hence taking limits, we get Fatou's lemma.

	3. Since $X_n + Y$, $Y - X_n$ give a sequence of random variables which are non-negative,
	\begin{align*}
		\mathbb{E}[X + Y | \mathcal{G}] = \mathbb{E}[\liminf_n (X_n + Y) | \mathcal{G}] \leq \liminf_{n \to \infty} \mathbb{E}[X_n + Y | \mathcal{G}]
	\end{align*}
	almost-surely, and similarly
	\[
	\mathbb{E}[Y - X | \mathcal{G}] = \mathbb{E}[ \liminf_{n \to \infty}(Y - X_n) | \mathcal{G}] \leq \liminf_{n \to \infty} \mathbb{E}[Y - X_n | \mathcal{G}]
	\]
	almost-surely. Combining these inequalities,
	\[
	\limsup_{n \to \infty} \mathbb{E}[X_n | \mathcal{G}] \leq \mathbb{E}[X|\mathcal{G}] \leq \liminf_{n \to \infty} \mathbb{E}[X_n | \mathcal{G}].
	\]
	This can only hold if $\lim_{n \to \infty} \mathbb{E}[X_n | \mathcal{G}] = \mathbb{E}[X | \mathcal{G}]$, almost-surely.

	4. We use the fact that a convex function can be written as a supremum of countably many affine functions:
	\[
	\phi(x) = \sup_i (a_i x + b_i).
	\]
	Hence we get
	\[
	\mathbb{E}[\phi(X) | \mathcal{G}] \geq a_i \mathbb{E}[X | \mathcal{G}] + b_i
	\]
	for all $i$ almost-surely, hence
	\[
	\mathbb{E}[\phi(X) | \mathcal{G}] \geq \sup_i (a_i \mathbb{E}[X | \mathcal{G}] + b_i) = \phi(\mathbb{E}[X | \mathcal{G}]),
	\]
	almost-surely.

	In particular, for $1 \leq p < \infty$,
	\begin{align*}
		\|\mathbb{E}[X | \mathcal{G}]\|_p^p &= \mathbb{E}[ | \mathbb{E}[X|\mathcal{G}]|^p] \leq \mathbb{E}[ \mathbb{E}[|X|^p | \mathcal{G}]] \\
						    &= \mathbb{E}[|X|^p] = \|X\|_p^p.
	\end{align*}
\end{proofbox}

\begin{proposition}[Tower Property]
	Let $\mathcal{H} \subseteq \mathcal{G} \subseteq \mathcal{F}$ be $\sigma$-algebras, and $X \in \mathcal{L}^1$. Then,
	\[
	\mathbb{E}[\mathbb{E}[X | \mathcal{G}] | \mathcal{H}] = \mathbb{E}[X | \mathcal{H}]
	\]
	almost-surely.
\end{proposition}

\begin{proofbox}
	Note that $\mathbb{E}[\mathbb{E}[X|\mathcal{G}]|\mathcal{H}]$ is $\mathcal{H}$-measurable. For $A \in \mathcal{H}$, we have
	\begin{align*}
		\mathbb{E}[\mathbb{E}[\mathbb{E}[X | \mathcal{G}]|\mathcal{ H}] \mathbbm{1}_{A}] &= \mathbb{E}[\mathbb{E}[X | \mathcal{G}] \mathbbm{1}_{A}] \\
												 &= \mathbb{E}[X \mathbbm{1}_{A}] = \mathbb{E}[\mathbb{E}[X|\mathcal{H}]\mathbbm{1}_{A}],
	\end{align*}
	since $A \in \mathcal{G}$.
\end{proofbox}

\begin{proposition}[Taking out what is known]
	Let $X \in \mathcal{L}^1$, $\mathcal{G} \subseteq \mathcal{F}$ a $\sigma$-algebra. If $Y$ is bounded and $\mathcal{G}$-measurable, then
	\[
		\mathbb{E}[XY|\mathcal{G}] = \mathbb{E}[X|\mathcal{G}] Y \qquad \text{\normalfont{a.s.}}
	\]
\end{proposition}

\begin{proofbox}
	$Y$ is $\mathcal{G}$-measurable, so $\mathbb{E}[X | \mathcal{G}]Y$ is $\mathcal{G}$-measurable.

	For $A \in \mathcal{G}$,
	\begin{align*}
		\mathbb{E}[(\mathbb{E}[X|\mathcal{G}]Y)\mathbbm{1}_{A}] &= \mathbb{E}[\mathbb{E}[X|\mathcal{G}](Y \mathbbm{1}_{A})] = \mathbb{E}[X Y \mathbbm{1}_{A}].
	\end{align*}
\end{proofbox}

% lecture 4

\begin{definition}
	Let $\mathcal{A}$ be a collection of subsets of $\Omega$. Then $\mathcal{A}$ is a \emph{$\pi$-system}\index{$\pi$-system} if for all $A, B \in \mathcal{A}$, $A \cap B \in \mathcal{A}$, and $\emptyset \in \mathcal{A}$.
\end{definition}

\begin{theorem}
	Let $\mu_1, \mu_2$ be measures on $(E, \mathcal{E})$. Suppose that $\mathcal{A}$ is a $\pi$-system which generates $\mathcal{E}$, and
	\[
	\mu_1(A) = \mu_2(A)
	\]
	for all $A \in \mathcal{A}$, and $\mu_1(E) = \mu_2(E)$ is finite.

	Then $\mu_1 = \mu_2$.
\end{theorem}

\begin{proposition}
	Let $X \in \mathcal{L}^1$, and $\mathcal{G}, \mathcal{H} \subseteq \mathcal{F}$ be $\sigma$-algebras. If $\sigma(X, \mathcal{G})$ are independent of $\mathcal{H}$, then
	\[
		\mathbb{E}[X|\sigma(\mathcal{G}, \mathcal{H})] = \mathbb{E}[X|\mathcal{G}] \qquad \text{\normalfont{a.s.}}
	\]
\end{proposition}

\begin{proofbox}
	Without loss of generality, $X \geq 0$, since the general case comes from decomposing $X = X^+ - X^-$. Let $A \in \mathcal{G}$, $B \in \mathcal{H}$. Then,
	\begin{align*}
		\mathbb{E}[\mathbb{E}[X|\sigma(\mathcal{G},\mathcal{H})]\mathbbm{1}_{A \cap B}] &= \mathbb{E}[X \mathbbm{1}_{A \cap B}] = \mathbb{E}[X \mathbbm{1}_{A}]\mathbb{P}(B) \text{ (independence)} \\
												&= \mathbb{E}[\mathbb{E}[X|\mathcal{G}] \mathbbm{1}_{A}] \mathbb{P}(B) = \mathbb{E}[\mathbb{E}[X|\mathcal{G}] \mathbbm{1}_{A \cap B}].
	\end{align*}
	Define two measures
	\begin{align*}
		\mu_1(F) &= \mathbb{E}[\mathbb{E}[X|\mathcal{G}] \mathbbm{1}_{F}], \\
		\mu_2(F) &= \mathbb{E}[\mathbb{E}[X | \sigma(\mathcal{G}, \mathcal{H})] \mathbbm{1}_{F}].
	\end{align*}
	These are measures on $\mathcal{F}$ which agree on the $\pi$-system $\{A \cap B \mid A \in \mathcal{G}, B \in \mathcal{H}\}$, generating $\sigma(\mathcal{G}, \mathcal{H})$. Moreover since $X \in \mathcal{L}^1$,
	\[
	\mu_1(F) = \mathbb{E}[\mathbb{E}[X|\mathcal{G}]] = \mathbb{E}[X] = \mu_2(F) < \infty.
	\]
	We can now use uniqueness of measures to see these two measures agree on $\sigma(\mathcal{G}, \mathcal{H})$, which can only occur if
	\[
		\mathbb{E}[X|\mathcal{G}] = \mathbb{E}[X|\sigma(\mathcal{G}, \mathcal{H})] \qquad \text{a.s.}
	\]
\end{proofbox}

\subsection{Examples of Conditional Expectation}%
\label{sub:exp}

\begin{exbox}[Gaussians]
Let $(X, Y)$ be a Gaussian random vector in $\mathbb{R}^2$. Our goal is to compute
\[
X' = \mathbb{E}[X|\mathcal{G}],
\]
where $\mathcal{G} = Y$. Since $X'$ is a $\mathcal{G}$-measurable function, there exists a Borel measurable function $f$ so that $X' = f(Y)$. We want to find $f$.

We guess that $X' = aY + b$, for $a, b \in \mathbb{R}$. Then,
\[
a \mathbb{E}[Y] + b = \mathbb{E}[\mathbb{E}[X|\mathcal{G}]] = \mathbb{E}[X],
\]
and moreover
\begin{align*}
	\mathbb{E}[(X - X') Y] &= 0 \\
	\implies \Cov(X - X', Y) &= 0 \\
	\implies \Cov(X, Y) = \Cov(X', Y) &= a \Var(Y).
\end{align*}
We can now take $a$ to satisfy this equation, so $\Cov(X - X', Y) = 0$.

But since $(X - X', Y)$ is a Gaussian random variable with covariance $0$, $X - X'$ and $Y$ are independent.

Suppose that $Z$ is a $\sigma(Y)$-measurable random variable. Then $Z$ is independent of $X - X'$, so
\[
\mathbb{E}[(X - X')Z] = \mathbb{E}[X - X']\mathbb{E}[Z] = 0.
\]
This shows the projection property. Hence
\[
\mathbb{E}[X | \mathcal{G}] = aY + b,
\]
where $a, b$ are determined as before.
\end{exbox}

\begin{exbox}[Conditional Density Functions]
	Suppose that $X, Y$ are random variables with a joint density function $f_{X, Y}$ on $\mathbb{R}^2$. Let $h : \mathbb{R} \to \mathbb{R}$ be Borel measurable so that $h(X)$ is integrable.

	Our goal is to compute
	\[
	\mathbb{E}[h(X) | Y] = \mathbb{E}[h(X) | \sigma(Y)].
	\]
	The density for $Y$ is given by
	\[
	f_Y(y) = \int_{\mathbb{R}} f_{X, Y} (x, y) \diff x.
	\]
	Let $g$ be a bounded, measurable function. Then,
	\begin{align*}
		\mathbb{E}[h(X) g(Y)] &= \int h(x) g(y) f_{X, Y}(x, y) \diff x \diff y \\
				      &= \int \left( \int h(x) \frac{f_{X, Y}(x, y)}{f_{Y}(y)} \diff x \right) g(y) f_{Y}(y) \diff y,
	\end{align*}
	where if $f_Y(y) = 0$, the inner integral is 0. We set
	\[
	\phi(y) =
	\begin{dcases}
		\int h(x) f_{X, Y}(x, y)/f_Y(y) \diff x & \text{if } f_Y(y) > 0,\\
		\;0 & \text{else}.
	\end{dcases}
	\]
	Then,
	\[
		\mathbb{E}[h(X)|Y] = \phi(Y) \qquad \text{a.s.}
	\]
	since $\phi(Y)$ is $\sigma(Y)$-measurable and satisfies the property defining the conditional expectation.

	We interpret this computation as giving that
	\[
	\mathbb{E}[h(X)|Y] = \int_{\mathbb{R}}h(x) \nu(Y, \diff x),
	\]
	where
	\begin{align*}
		\nu(y, \diff x) &= \frac{f_{X, Y}(x, y)}{f_Y(y)} \mathbbm{1}_{f_Y(y) \geq 0} \diff x \\
				&= f_{X|Y}(x|y) \diff x.
	\end{align*}
	$\nu(y, \diff x)$ gives the \emph{conditional distribution}\index{conditional distribution} of $X$ given $Y = y$, and $f_{X|Y}(x|y)$ is the \emph{conditional density function}\index{conditional density function} of $X$ given $Y = y$.
\end{exbox}

In this case, the conditional expectation corresponds to an actual expectation. This corresponds to a regular conditional probability distribution.

Also note $f_{X|Y}(x|y)$ is only defined up to a set of measure $0$.

% lecture 5

\newpage

\section{Discrete Time Martingales}%
\label{sec:dtm}

Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space, and $(E, \mathcal{E})$ a measurable space.

A sequence of random variables $X = (X_n)$ with values in $E$ is a (discrete time) \emph{stochastic process}\index{stochastic process}. A \emph{filtration}\index{filtration} $(\mathcal{F}_n)$ is a sequence of $\sigma$-algebras with $\mathcal{F}_n \subseteq \mathcal{F}_{n+1} \subseteq \mathcal{F}$.

The \emph{natural filtration}\index{natural filtration} $(\mathcal{F}_n^X)$ associated with $X$ is
\[
\mathcal{F}_n^X = \sigma(X_1, \ldots, X_n).
\]
We say that $X$ is \emph{adapted}\index{adapted} to $(\mathcal{F}_n)$ if $X_n$ is $\mathcal{F}_n$-measurable for all $n$. $X$ is always adapted to its natural filtration. Say that $X$ is \emph{integrable}\index{integrable process} if $X_n \in \mathcal{L}^1$ for all $n$.

\begin{definition}
	Let $(\Omega, \mathcal{F}, (\mathcal{F}_n), \mathbb{P})$ be a filtered probability space and let $X$ be a stochastic process which is integrable, adapted and real valued.
	\begin{itemize}
		\item $X$ is a \emph{martingale}\index{martingale} (MG) if
			\[
				\mathbb{E}[X_n| \mathcal{F}_m] = X_m \qquad \text{a.s.}
			\]
			whenever $n \geq m$.
		\item $X$ is a \emph{supermartingale}\index{supermartingale} ($\sup$ MG) if
			\[
				\mathbb{E}[X_n|\mathcal{F}_m] \leq X_m \qquad \text{a.s.}
			\]
			whenever $n \geq m$.
		\item $X$ is a \emph{submartingale}\index{submartingale} (sub MG) if
			\[
				\mathbb{E}[X_n|\mathcal{F}_m] \geq X_m \qquad \text{a.s.}
			\]
			whenever $n \geq m$.
	\end{itemize}
\end{definition}

If $X$ is a MG/sup MG/sub MG, then it is also a MG/sup MG/sub MG with respect to its natural filtration.

\begin{exbox}
	\begin{enumerate}
		\item Say
			\[
			X_n = \sum_{i = 1}^n \xi_i,
			\]
			where $(\xi_i)$ are IID, integrable and $\mathbb{E}[\xi_i] = 0$.
		\item We could also have
			\[
			X_n = \prod_{i = 1}^n \xi_i,
			\]
			where $(\xi_i)$ are IID, integrable and $\mathbb{E}[\xi_i] = 1$.
		\item Choosing
			\[
			X_n = \mathbb{E}[Z | \mathcal{F}_n],
			\]
			where $\mathcal{F} \in \mathcal{L}^1$ and $(\mathcal{F}_n)$ is a filtration, this gives a martingale.
	\end{enumerate}	
\end{exbox}

Martingales are very useful for:
\begin{itemize}
	\item computations (optimal stopping theorem),
	\item bounds (Doob's inequalities),
	\item proving theorems (martingale convergence theorem).
\end{itemize}

\subsection{Stopping Times}%
\label{sub:stop}

\begin{definition}
	Let $(\Omega, \mathcal{F}, (\mathcal{F}_n), \mathbb{P})$ be a filtered probability space. A \emph{stopping time}\index{stopping time} is a random variable $T : \Omega \to \mathbb{Z}_+ \cup \{\infty\}$ with
	\[
		\{T \leq n\} \in \mathcal{F}_n,
	\]
	for all $n$.
\end{definition}

This is equivalent to $\{T = n\} \in \mathcal{F}_n$, for discrete time.

\begin{exbox}
	\begin{enumerate}
		\item Constant (deterministic) times.
		\item First hitting times: if $(X_n)$ is an adapted stochastic process with values in $\mathbb{R}$, and $A \in \mathcal{B}(\mathbb{R})$, then
			\[
			T_A = \inf\{n \geq 0 \mid X_n \in \mathcal{A}\}.
			\]
		\item Last exit times are not always stopping times.
	\end{enumerate}	
\end{exbox}

\begin{proposition}
	Suppose that $S, T, (T_n)$ are stopping times on $(\Omega, \mathcal{F}, (\mathcal{F}_n), \mathbb{P})$. Then:
	\begin{align*}
		&S \wedge T, &  &S \vee T & & \inf T_n, \\
		&\sup T_n, & &\liminf T_n, & & \limsup T_n,
	\end{align*}
	are stopping times
\end{proposition}

\begin{definition}
	Let $T$ be a stopping time on $(\Omega, \mathcal{F}, (\mathcal{F}_n), \mathbb{P})$. Then
	\[
		\mathcal{F}_T = \{A \in \mathcal{F} \mid A \cap \{T \leq n\} \in \mathcal{F}_n \text{ for all }n \}
	\]
	is called the \emph{stopped $\sigma$-algebra}\index{stopped $\sigma$-algebra}.
\end{definition}

For $T = n$ deterministic, $\mathcal{F}_T = \mathcal{F}_n$. For $X$ a stochastic process, $X_T = X_{T(\omega)}(\omega)$, whenever $T(\omega) < \infty$.

The \emph{stopped process}\index{stopped process} $X^T$ is defined by
\[
X^T_n = X_{ n \wedge T}.
\]

\begin{proposition}
	Let $S, T$ be stopping times and $X$ an adapted process. Then,
	\begin{enumerate}[\normalfont(i)]
		\item If $S \leq T$, then $\mathcal{F}_S \subseteq \mathcal{F}_T$.
		\item $X_T \mathbbm{1}_{\{T < \infty\}}$ is $\mathcal{F}_T$-measurable.
		\item $X^T$ is adapted.
		\item If $X$ is integrable, so is $X^T$.
	\end{enumerate}
\end{proposition}

\begin{proofbox}
	
	
	(i) This is immediate from definition.

	(ii) Take $A \in \mathcal{E}$, then
	\begin{align*}
		\{X_T \mathbbm{1}_{\{T < \infty\}} \in A \} \cap \{T \leq n\} = \bigcup_{k = 0}^n (\{X_k \in A\} \cap \{T = k\}) \in \mathcal{F}_n.
	\end{align*}
	Since $X$ is adapted, $\{T = k\} \in \mathcal{F}_k$.

	(iii) For all $n$, $X_{T \wedge n}$ is $\mathcal{F}_{T \wedge n}$-measurable by (ii), so they are $\mathcal{F}_n$-measurable by (i).

	(iv) Note
	\begin{align*}
		\mathbb{E}[|X_{T \wedge n}|] &= \mathbb{E}\left[ \sum_{k = 0}^{n-1} |X_k| \mathbbm{1}(T = k) \right] + \mathbb{E}\left[ \sum_{k = n}^\infty |X_n| \mathbbm{1}(T = k) \right] \\
					     &\leq \sum_{k = 0}^n \mathbb{E}[|X_k|] < \infty.
	\end{align*}
\end{proofbox}

\begin{theorem}[Optional Stopping Theorem]
	Let $(X_n)$ be a MG. Then:
	\begin{enumerate}[\normalfont(i)]
		\item If $T$ is a stopping time, then $X^T$ is a MG, hence
			\[
			\mathbb{E}[X_{T \wedge n}] = \mathbb{E}[X_0]
			\]
			for all $n$.
		\item If $S \leq T$ are bounded stopping times, then
			\[
				\mathbb{E}[X_T|\mathcal{F}_S] = X_S \qquad \text{\normalfont{a.s.}}
			\]
		\item If $S \leq T$ are bounded stopping times, then
			\[
			\mathbb{E}[X_T] = \mathbb{E}[X_S].
			\]
		\item If there exists an integrable random variable $Y$ such that $|X_n| \leq Y$ for all $n$, then for all almost-surely finite stopping times $T$,
			\[
			\mathbb{E}[X_T] = \mathbb{E}[X_0].
			\]
		\item If $X$ has bounded increments, so $|X_{n+1} - X_n| \leq M$, and $T$ is a stopping time with finite expectation, then
			\[
			\mathbb{E}[X_T] = \mathbb{E}[X_0].
			\]
	\end{enumerate}
\end{theorem}

\begin{proofbox}
	

	(i) By the tower property, we only need to show that
	\[
		\mathbb{E}[X_{T \wedge n} | \mathcal{F}_{n-1}] = X_{T \wedge (n-1)} \qquad \text{a.s.}
	\]
	We have
	\begin{align*}
		\mathbb{E}[X_{T \wedge n}|\mathcal{F}_{n-1}] &= \mathbb{E}\left[ \sum_{k = 0}^{n-1} X_k \mathbbm{1}(T = k) \bigm| \mathcal{F}_{n-1} \right] + \mathbb{E}[X_n \mathbbm{1}(T > n-1) | \mathcal{F}_{n-1}] \\
							     &= X_T \mathbbm{1}_{(T < n-1)} + X_{n-1} \mathbbm{1}_{(T > n - 1)} = X_{T \wedge (n - 1)}.
	\end{align*}
	Since $(T > n-1) \in \mathcal{F}_{n-1}$, $\mathbb{E}[X_n|\mathcal{F}_{n-1}] = X_{n-1}$.

	(ii) Suppose that $T \leq n$, and $S \leq T$. Then,
	\begin{align*}
		X_T &= (X_T - X_{T-1}) + \cdots + (X_{S+1} - X_S) + X_S \\
		    &= X_S + \sum_{k = 0}^n (X_{k+1} - X_k) \mathbbm{1}_{S \leq k < T}.
	\end{align*}
	Let $A \in \mathcal{F}_S$. Then,
	\begin{align*}
		\mathbb{E}[X_T \mathbbm{1}_{A}] &= \mathbb{E}[X_S \mathbbm{1}_{A}] + \mathbb{E}\left[ \sum_{k = 0}^n (X_{k+1} - X_k) \mathbbm{1}_{A} \mathbbm{1}(S \leq k < T) \right] \\
						&= \mathbb{E}[X_S \mathbbm{1}_{A}],
	\end{align*}
	since $\{S \leq k < T\} \cap A \in \mathcal{F}_k$ for all $k$, and $X$ is a MG.


	(iii) This follows from taking expectations in (ii).

	(iv) and (v) are on the example sheet.
\end{proofbox}


% lecture 6

\newpage

\printindex

\end{document}
