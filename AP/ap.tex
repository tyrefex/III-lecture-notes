\documentclass[12pt]{article}

\usepackage{ishn}

\makeindex[intoc]

\begin{document}

\hypersetup{pageanchor=false}
\begin{titlepage}
	\begin{center}
		\vspace*{1em}
		\Huge
		\textbf{III Advanced Probability}

		\vspace{1em}
		\large
		Ishan Nath, Michaelmas 2024

		\vspace{1.5em}

		\Large

		Based on Lectures by Prof. Jason Miller

		\vspace{1em}

		\large
		\today
	\end{center}
	
\end{titlepage}
\hypersetup{pageanchor=true}

\tableofcontents

\newpage

% lecture 1

\setcounter{section}{-1}

\section{Introduction}%
\label{sec:intro}

We will be following the lecture notes by Perla Sousi.

This course is divided into seven main topics:
\begin{enumerate}[label=Chapter \arabic*.]
	\item Conditional expectation. We have seen how to define $\mathbb{E}[Y | X = x]$ for $X$ a discrete-valued random variable, or one with continuous density wrt. the Lebesgue measure. We will define more generally $\mathbb{E}[Y|\mathcal{G}]$, for $\mathcal{G}$ a $\sigma$-algebra.
	\item Discrete-time martingales. A martingale is a sequence $(X_n)$ of integrable random variables such that $\mathbb{E}[X_{n+1} \mid X_1, \ldots, X_n] = X_n$, for all $n$. A basic example is a simple symmetric random walk. We will be looking at properties, bounds and computations.
	\item Continuous time processes. Examples are
		\begin{itemize}
			\item Martingales in continuous time.
			\item Some continuous continuous-time processes.
		\end{itemize}
	\item Weak convergence. This is a notion of convergence which extends convergence in distribution, and is related to other notions of convergence.
	\item Large deviations. Here we estimate how unlikely rare events are. An example if the sample mean for $(\xi_j)$ iid, with mean $\mu$, which converges to $\mu$ by SLLN. We can ask what the probability is that the sample mean deviates from $\mu$ by $\eps$, which can be answered by Cramer's theorem.
	\item Brownian motion. This is a fundamental object, which is a continuous time stochastic process. We will look at the definition, Markov processes, and its relationship with PDEs.
	\item Poisson random measures. This is the generalization of the standard Poisson process on $\mathbb{R}_+$.
\end{enumerate}

\newpage

\section{Conditional Expectation}%
\label{sec:cond_e}

Recall that a probability space $(\Omega, \mathcal{F}, \mathbb{P})$ consists of a set $\Omega$, a $\sigma$-algebra on $\Omega$, meaning:
\begin{itemize}
	\item $\Omega \in \mathcal{F}$,
	\item $A \in \mathcal{F} \implies A^{c} \in \mathcal{F}$,
	\item if $(A_n)$ is in $\mathcal{F}$, then $\bigcup A_n \in \mathcal{F}$,
\end{itemize}
and $\mathbb{P}$ is a probability measure on $(\Omega, \mathcal{F})$, meaning:
\begin{itemize}
	\item $\mathbb{P} : \mathcal{F} \to [0, 1]$ such that
	\item $\mathbb{P}(\Omega) = 1$, $\mathbb{P}(\emptyset) = 0$,
	\item if $(A_n)$ are pairwise disjoint in $\mathcal{F}$, then $\mathbb{P}(\bigcup A_n) = \sum \mathbb{P}(A_n)$.
\end{itemize}
An important $\sigma$-algebra is the Borel $\sigma$-algebra $\mathcal{B}$, which for $\mathbb{R}$ is the intersection of all $\sigma$-algebra on $\mathbb{R}$ which contain the open sets in $\mathbb{R}$.

Recall that $X : \Omega \to \mathbb{R}$ is a random variable if $X^{-1}(U) \in \mathcal{F}$ for all $U \subseteq \mathbb{R}^n$ open. This is equivalent to $X^{-1}(B) \in \mathcal{F}$ for all $B \in \mathcal{B}$.

\begin{definition}
	Suppose that $\mathcal{A}$ is a collection of sets in $\Omega$. Then
	\[
		\sigma(\mathcal{A}) = \bigcap \{ \mathcal{E} \mid \mathcal{E} \text{ is a $\sigma$-algebra containing } \mathcal{A}\}.
	\]
\end{definition}

If $(X_i)$ is a collection of random variables, then
\[
	\sigma(X_i \mid i \in I) = \sigma(\{ \omega \in \Omega \mid X_i(\omega) \in B\}, i \in I, B \in \mathcal{B}).
\]
This is the smallest $\sigma$-algebra which makes $X_i$ measurable for all $i \in I$.

Let $A \in \mathcal{F}$. Set
\[
\mathbbm{1}_{A}(x) = \mathbbm{1}(x \in A) = 
\begin{cases}
	1 & x \in A, \\
	0 & \text{else}.
\end{cases}
\]
We can define expectation as follows:
\begin{itemize}
	\item For $C_i \geq 0$, $A_i \in \mathcal{F}$, set
		\[
		\mathbb{E}\left[\sum_{i = 1}^n C_i \mathbbm{1}_{A_i} \right] = \sum_{i = 1}^n C_i \mathbb{P}(A_i).
		\]
	\item For $X \geq 0$, set $X_n = (2^{-n} \lfloor 2^n X \rfloor) \wedge n$, so that $X_n \uparrow X$ as $n \to \infty$. Set
		\[
		\mathbb{E}[X] = \lim_{n \to \infty} \mathbb{E}[X_n].
		\]
	\item If $X$ is a general random variable, write $X = X^+ - X^-$, where $X^+ = \max(X, 0)$, $X^- = \max(-X, 0)$. If $\mathbb{E}[X^+]$ or $\mathbb{E}[X^-]$ is finite, set
		\[
		\mathbb{E}[X] = \mathbb{E}[X^+] - \mathbb{E}[X^-].
		\]	
\end{itemize}
Call $X$ integrable if $\mathbb{E}[|X|] < \infty$.

Recall that if $A, B \in \mathcal{F}$ with $\mathbb{P}(B) > 0$, then
\[
\mathbb{P}(A \mid B) = \mathbb{P}(A \cap B)/\mathbb{P}(B).
\]
If $X$ is integrable and $A \in \mathcal{F}$ with $\mathbb{P}(A) > 0$, then
\[
\mathbb{E}[X | A] = \mathbb{E}[X \mathbbm{1}_{A}] / \mathbb{P}(A).
\]
Our goal is to extend this definition to
\[
\mathbb{E}[X | \mathcal{G}],
\]
where $\mathcal{G}$ is a $\sigma$-algebra in $\mathcal{F}$. The main idea is that $\mathbb{E}[X | \mathcal{G}]$ is the best prediction of $X$ given $\mathcal{G}$.

Hence it should be a $\mathcal{G}$-measurable random variable $Y$, which minimizes
\[
\mathbb{E}[(X - Y)^2].
\]

% lecture 2

\subsection{Discrete Case}%
\label{sub:discrete_cond}


As a warm-up, we consider the discrete case.

Suppose that $X$ is integrable, $(B_i)$ is countable and disjoint with $\Omega = \bigcup B_i$, and $\mathcal{G} = (B_i, i \in I)$. Set $X' = \mathbb{E}[X | \mathcal{G}]$ by
\[
X' = \sum_{i \in I} \mathbb{E}[X | B_i ] \mathbbm{1}_{B_i},
\]
with the convention that $\mathbb{E}[X | B_i] = 0$ if $\mathbb{P}(B_i) = 0$. If $\omega \in \Omega$, then
\[
X'(\omega) = \sum_i \mathbb{E}[X | B_i] \mathbbm{1}(\omega \in B_i).
\]
We look at some important properties of $X'$.
\begin{enumerate}
	\item $X'$ is $\mathcal{G}$-measurable, as it is a linear combination of the $\mathcal{G}$-measurable random variables $\mathbbm{1}_{B_i}$.
	\item $X'$ is integrable, as
		\[
		\mathbb{E}[|X'|] \leq \sum_{i \in I} \mathbb{E}[|X| \mathbbm{1}_{B_i}] = \mathbb{E}[|X|] < \infty.
		\]
	\item If $G \in \mathcal{G}$, then
		\[
		\mathbb{E}[X \mathbbm{1}_{G}] = \mathbb{E}[X' \mathbbm{1}_{G}].
		\]
\end{enumerate}

\subsection{Existence and Uniqueness}%
\label{sub:e_u}

We need a couple of important facts from measure theory:

\begin{theorem}[Monotone Convergence Theorem]
	If $(X_n)$ is a sequence of random variables with $X_n \geq 0$ for all $n$ and $X_n \uparrow X$ almost-surely, then $\mathbb{E}[X_n] \uparrow \mathbb{E}[X]$.
\end{theorem}

\begin{theorem}[Dominated Convergence Theorem]
	Suppose that $(X_n)$ is a sequence of random variables with $X_n \to X$ almost-surely and $|X_n| \leq Y$ for all $n$ where $Y$ is an integrable random variable, then $\mathbb{E}[X_n] \to \mathbb{E}[X]$ as $n \to \infty$.
\end{theorem}

The construction of $\mathbb{E}[X | \mathcal{G}]$ requires a few steps; in the case that $X \in L^2$, then this can be defined by orthogonal projection.

We need to recall a few things about $L^p$ spaces. Suppose that $p \in [1, \infty)$, and $X$ is a random variables in $(\Omega, \mathcal{F})$. Then,
\[
\|X\|_p = \mathbb{E}[|X|^p]^{1/p}.
\]
Now
\[
	L^p = L^p(\Omega, \mathcal{F}, \mathbb{P}) = \{X \text{ with } \|X\|_p < \infty\}.
\]
For $p = \infty$,
\[
	\|X\|_\infty = \inf\{ \lambda \mid |X| \leq \lambda \text{ almost surely}\},
\]
and
\[
	L^\infty = L^\infty(\Omega, \mathcal{F}, \mathbb{P}) = \{X \text{ with } \|X\|_\infty < \infty\}.
\]
Two random variables in $L^p$ are equivalent if they agree almost-surely. Then $\mathcal{L}^p$ is the set of equivalence classes under this equivalence relation.

\begin{theorem}
	The space $(\mathcal{L}^2, \|\cdot\|_2)$ is a Hilbert space with inner product
	\[
	\langle X, Y \rangle = \mathbb{E}[XY].
	\]
	If $\mathcal{H}$ is a closed subspace of $\mathcal{L}^2$, then for all $X \in \mathcal{L}^2$, there exists $Y \in \mathcal{H}$ such that
	\[
	\|X - Y\|_2 = \inf_{Z \in \mathcal{H}}\|X - Z\|_2,
	\]
	and  $\langle X - Y, Z \rangle = 0$ for all $Z \in \mathcal{H}$. $Y$ is the \emph{orthogonal projection}\index{orthogonal projection} of $X$ onto $\mathcal{H}$.
\end{theorem}

\begin{theorem}
	Let $X$ be an integrable random variable, and $\mathcal{G} \subseteq \mathcal{F}$ a $\sigma$-algebra. Then there exists a random variable $Y$ such that:
	\begin{enumerate}[\normalfont(i)]
		\item $Y$ is $\mathcal{G}$-measurable.
		\item $Y$ is integrable, with
			\[
			\mathbb{E}[X \mathbbm{1}_{A}] = \mathbb{E}[Y \mathbbm{1}_{A}]
			\]
			for all $A \in \mathcal{G}$.
	\end{enumerate}
	If $Y'$ satisfies these two properties, then $Y = Y'$ almost surely. $Y$ is called (a version of) the \emph{conditional expectation}\index{conditional expectation} of $X$ given $\mathcal{G}$, and we write
	\[
	Y = \mathbb{E}[X | \mathcal{G}].
	\]
	If $\mathcal{G} = \sigma(G)$ for a random variable $G$, we write $Y = \mathbb{E}[X | G]$.
\end{theorem}

\begin{proofbox}
	We begin by proving uniqueness. Suppose $Y, Y'$ are two conditional expectation. Then
	\[
		A = \{Y > Y'\} \in \mathcal{G},
	\]
	so by (ii),
	\[
	\mathbb{E}[(Y - Y') \mathbbm{1}_{A}] = \mathbb{E}[X \mathbbm{1}_{A}] - \mathbb{E}[X \mathbbm{1}_{A}] = 0,
	\]
	but the LHS is non-negative, so the only way this can hold is if $Y \leq Y'$ almost surely. Similarly, $Y \geq Y'$ almost surely, so $Y = Y'$ almost surely.

	Now we are ready to tackle existence. The first step is by restricting to $\mathcal{L}^2$ function.

	Take $X \in \mathcal{L}^2$. Note that $\mathcal{L}^2(\Omega, \mathcal{G}, \mathbb{P})$ is a closed subspace of $\mathcal{L}^2(\Omega, \mathcal{F}, \mathbb{P})$.

	Set $Y = \mathbb{E}[X | \mathcal{G}]$ to be the orthogonal projections of $X$ onto $\mathcal{L}^2(\Omega, \mathcal{G}, \mathbb{P})$.

	It is automatically true that (i) holds. Suppose $A \in \mathcal{G}$. Then
	\[
	\mathbb{E}[(X - Y) \mathbbm{1}_{A}] = 0,
	\]
	by orthogonality, since $\mathbbm{1}_{A}$ is $\mathcal{G}$-measurable. So
	\[
	\mathbb{E}[X \mathbbm{1}_{A}] = \mathbb{E}[Y \mathbbm{1}_{A}],
	\]
	so (ii) holds. Finally $Y$ is integrable as it is specified to be in $\mathcal{L}^2$.


	As an aside, notice if $X \geq 0$, then $Y = \mathbb{E}[X | \mathcal{G}] \geq 0$ almost-surely. Since $\{Y < 0\} \in \mathcal{G}$ and
	\[
		\mathbb{E}[Y \mathbbm{1}_{\{Y < 0\}}] = \mathbb{E}[X \mathbbm{1}_{\{Y < 0\}}] \geq 0,
	\]
	which can only happen if $\mathbb{P}(Y < 0) = 0$.

	Our second step is to prove this for $X \geq 0$. In this case, let $X_n = X \wedge n$. Then $X_n \in \mathcal{L}^2$ for all $n$, so there exists $\mathcal{G}$-measurable random variables $Y_n$ so that
	\[
	\mathbb{E}[Y_n \mathbbm{1}_{A}] = \mathbb{E}[(X \wedge n) \mathbbm{1}_{A}],
	\]
	for all $A \in \mathcal{G}$. Now $(X_n)$ is increasing in $n$, so $(Y_n)$ are also increasing by the above aside. Set
	\[
	Y = \limsup_{n} Y_n.
	\]
	We now have to check the definitions. Note $Y$ is $\mathcal{G}$-measurable as it is a limsup of $\mathcal{G}$-measurable functions. So we check the second definition.

	For $A \in \mathcal{G}$,
	\[
		\mathbb{E}[Y \mathbbm{1}_{A}] \overset{MCT}= \lim_n \mathbb{E}[Y_n \mathbbm{1}_{A}] = \lim_n \mathbb{E}[(X \wedge n) \mathbbm{1}_{A}] \overset{MCT}= \mathbb{E}[X \mathbbm{1}_{A}].
	\]
	We did not check integrability, but notice that setting $A = G$, $\mathbb{E}[Y] = \mathbb{E}[X]$, and $X$ is non-negative, hence so is $Y$. Thus $Y$ is integrable.

	Finally we prove the result for $X \in \mathcal{L}^1$. We can apply step 2 to $X^+$ and $X^-$, and so can set
	\[
	\mathbb{E}[X|\mathcal{G}] = \mathbb{E}[X^+|\mathcal{G}] - \mathbb{E}[X^-|\mathcal{G}].
	\]
	This is $\mathcal{G}$-measurable and integrable as it is the difference of two $\mathcal{G}$-measurable and integrable random variables, and satisfies (ii) since both of the random variables satisfy (ii).
\end{proofbox}

% lecture 3

\begin{remark}
	\begin{itemize}
		\item[]
		\item The proof also works for $X \geq 0$, and not necessarily integrable. Then $\mathbb{E}[X | \mathcal{G}]$ is not necessarily integrable.
		\item The property
			\[
			\mathbb{E}[\mathbb{E}[X|\mathcal{G}]\mathbbm{1}_{A}] = \mathbb{E}[X \mathbbm{1}_{A}]
			\]
			for all $A \in \mathcal{G}$, is equivalent to
			\[
			\mathbb{E}[\mathbb{E}[X|\mathcal{G}]Y] = \mathbb{E}[XY],
			\]
			for all $Y$ bounded and $\mathcal{G}$-measurable.
	\end{itemize}
\end{remark}

\subsection{Properties of Expectation}%
\label{sub:eepy}

\begin{definition}
	A collection of $\sigma$-algebras $(\mathcal{G}_i)$ in $\mathcal{G}$ is \emph{independent}\index{independent} if whenever $G_i \in \mathcal{G}_i$ and $i_1, \ldots, i_n$ distinct, then
	\[
	\mathbb{P}(G_{i_1} \cap \cdots \cap G_{i_n}) = \prod_{k = 1}^n \mathbb{P}(G_{ik}).
	\]
	We say that a random variable $X$ is \emph{independent} of a $\sigma$-algebra $\mathcal{G}$ if $\sigma(X)$ is independent of $\mathcal{G}$.
\end{definition}

\begin{proposition}
	Let $X, Y \in \mathcal{L}^2$, and $\mathcal{G} \subseteq \mathcal{F}$ a $\sigma$-algebra. Then:
	\begin{enumerate}[\normalfont(i)]
		\item $\mathbb{E}[\mathbb{E}[X|\mathcal{G}]] = \mathbb{E}[X]$.
		\item If $X$ is $\mathcal{G}$-measurable, then $\mathbb{E}[X|\mathcal{G}] = X$ almost surely.
		\item If $X$ is independent of $\mathcal{G}$, then $\mathbb{E}[X|\mathcal{G}] = \mathbb{E}[X]$.
		\item If $X \geq 0$ almost surely, then $\mathbb{E}[X|\mathcal{G}] \geq 0$ almost surely.
		\item For any $\alpha, \beta \in \mathbb{R}$, $\mathbb{E}[\alpha X + \beta Y \mid \mathcal{G}] = \alpha \mathbb{E}[X|\mathcal{G}] + \beta \mathbb{E}[Y|\mathcal{G}]$.
		\item $|\mathbb{E}[X|\mathcal{G}]| \leq \mathbb{E}[|X||\mathcal{G}]$ almost-surely.
	\end{enumerate}
\end{proposition}

\begin{theorem}[Fatou's Lemma]
	If $(X_n)$ is a sequence of random variables with $X_n \geq 0$, then
	\[
	\mathbb{E}[\liminf_n X_n] \leq \liminf_n \mathbb{E}[X_n].
	\]
\end{theorem}

\begin{theorem}[Jensen's Inequality]
	Let $X \in \mathcal{L}^2$ and let $\phi : \mathbb{R} \to (-\infty, \infty]$ be a convex function. Then,
	\[
	\mathbb{E}[\phi(X)] \geq \phi(\mathbb{E}[X]).
	\]
\end{theorem}

\begin{proposition}
	Let $\mathcal{G} \subseteq \mathcal{F}$ be a $\sigma$-algebra.
	\begin{enumerate}[\normalfont1.]
		\item If $(X_n)$ is an increasing sequence of random variables with $X_n \geq 0$ for all $n$ and $X_n \uparrow X$, then
			\[
			\mathbb{E}[X_n | \mathcal{G}] \uparrow \mathbb{E}[X |\mathcal{G}],
			\]
			almost-surely (conditional MCT).
		\item If $X_n \geq 0$, then
			\[
			\mathbb{E}[\liminf_n X_n | \mathcal{G}] \leq \liminf_n \mathbb{E}[X_n | \mathcal{G}]
			\]
			(conditional Fatou's lemma).
		\item If $X_n \to X$ and $|X_n| \leq Y$ almost-surely for all $n$ and $Y \in \mathcal{L}^1$, then
			\[
			\lim_{n \to \infty} \mathbb{E}[X_n | \mathcal{G}] = \mathbb{E}[X | \mathcal{G}],
			\]
			almost-surely (conditional DCT).
		\item If $X \in \mathcal{L^1}$ and $\phi : \mathbb{R} \to (-\infty, \infty]$ is convex such that either $\phi(X) \in \mathcal{L}^1$ or $\phi(X) \geq 0$, then:
			\[
			\mathbb{E}[\phi(X) | \mathcal{G}] \geq \phi(\mathbb{E}[X | \mathcal{G}])
			\]
			almost surely (convex Jensen's). In particular, for all $1 \leq p < \infty$,
			\[
			\|\mathbb{E}[X|\mathcal{G}]\|_p \leq \|X\|_p.
			\]
	\end{enumerate}
\end{proposition}

\begin{proofbox}


	1. Let $Y_n$ be a version of of $\mathbb{E}[X_n | \mathcal{G}]$. Since $0 \leq X_n \uparrow X$ as $n \to \infty$, we have that $Y_n \geq 0$ and are increasing.

	Define $Y = \limsup_{n \to \infty} Y_n$. We will show that $ = \mathbb{E}[X|\mathcal{G}]$.
	\begin{itemize}
		\item $Y$ is $\mathcal{G}$-measurable as it is a $\limsup$ of $\mathcal{G}$-measurable random variables.
		\item For $A \in \mathcal{G}$,
			\[
				\mathbb{E}[ X \mathbbm{1}_{A}] \overset{MCT}= \lim_{n \to \infty} \mathbb{E}[X_n \mathbbm{1}_{A}] = \lim_{n \to \infty} \mathbb{E}[Y_n \mathbbm{1}_{A}] \overset{MCT}= \mathbb{E}[Y \mathbbm{1}_{A}].
			\]
	\end{itemize}
	2. The sequence $\inf_{k \geq n} X_k$ is increasing in $n$. Moreover,
	\[
	\lim_{n \to \infty} \inf_{k \geq n} X_k = \liminf_{n \to \infty} X_n.
	\]
	By 1,
	\[
	\lim_{n \to \infty} \mathbb{E}[ \inf_{ k \geq n} X_k | \mathcal{G}] = \mathbb{E}[ \liminf_{n \to \infty} X_n | \mathcal{G}].
	\]
	But also,
	\[
	\mathbb{E}[\inf_{k \geq n} X_k |\mathcal{G}] \leq \inf_{k \geq n} \mathbb{E}[X_k | \mathcal{G}]
	\]
	almost-surely, by monotonicity. Hence taking limits, we get Fatou's lemma.

	3. Since $X_n + Y$, $Y - X_n$ give a sequence of random variables which are non-negative,
	\begin{align*}
		\mathbb{E}[X + Y | \mathcal{G}] = \mathbb{E}[\liminf_n (X_n + Y) | \mathcal{G}] \leq \liminf_{n \to \infty} \mathbb{E}[X_n + Y | \mathcal{G}]
	\end{align*}
	almost-surely, and similarly
	\[
	\mathbb{E}[Y - X | \mathcal{G}] = \mathbb{E}[ \liminf_{n \to \infty}(Y - X_n) | \mathcal{G}] \leq \liminf_{n \to \infty} \mathbb{E}[Y - X_n | \mathcal{G}]
	\]
	almost-surely. Combining these inequalities,
	\[
	\limsup_{n \to \infty} \mathbb{E}[X_n | \mathcal{G}] \leq \mathbb{E}[X|\mathcal{G}] \leq \liminf_{n \to \infty} \mathbb{E}[X_n | \mathcal{G}].
	\]
	This can only hold if $\lim_{n \to \infty} \mathbb{E}[X_n | \mathcal{G}] = \mathbb{E}[X | \mathcal{G}]$, almost-surely.

	4. We use the fact that a convex function can be written as a supremum of countably many affine functions:
	\[
	\phi(x) = \sup_i (a_i x + b_i).
	\]
	Hence we get
	\[
	\mathbb{E}[\phi(X) | \mathcal{G}] \geq a_i \mathbb{E}[X | \mathcal{G}] + b_i
	\]
	for all $i$ almost-surely, hence
	\[
	\mathbb{E}[\phi(X) | \mathcal{G}] \geq \sup_i (a_i \mathbb{E}[X | \mathcal{G}] + b_i) = \phi(\mathbb{E}[X | \mathcal{G}]),
	\]
	almost-surely.

	In particular, for $1 \leq p < \infty$,
	\begin{align*}
		\|\mathbb{E}[X | \mathcal{G}]\|_p^p &= \mathbb{E}[ | \mathbb{E}[X|\mathcal{G}]|^p] \leq \mathbb{E}[ \mathbb{E}[|X|^p | \mathcal{G}]] \\
						    &= \mathbb{E}[|X|^p] = \|X\|_p^p.
	\end{align*}
\end{proofbox}

\begin{proposition}[Tower Property]
	Let $\mathcal{H} \subseteq \mathcal{G} \subseteq \mathcal{F}$ be $\sigma$-algebras, and $X \in \mathcal{L}^1$. Then,
	\[
	\mathbb{E}[\mathbb{E}[X | \mathcal{G}] | \mathcal{H}] = \mathbb{E}[X | \mathcal{H}]
	\]
	almost-surely.
\end{proposition}

\begin{proofbox}
	Note that $\mathbb{E}[\mathbb{E}[X|\mathcal{G}]|\mathcal{H}]$ is $\mathcal{H}$-measurable. For $A \in \mathcal{H}$, we have
	\begin{align*}
		\mathbb{E}[\mathbb{E}[\mathbb{E}[X | \mathcal{G}]|\mathcal{ H}] \mathbbm{1}_{A}] &= \mathbb{E}[\mathbb{E}[X | \mathcal{G}] \mathbbm{1}_{A}] \\
												 &= \mathbb{E}[X \mathbbm{1}_{A}] = \mathbb{E}[\mathbb{E}[X|\mathcal{H}]\mathbbm{1}_{A}],
	\end{align*}
	since $A \in \mathcal{G}$.
\end{proofbox}

\begin{proposition}[Taking out what is known]
	Let $X \in \mathcal{L}^1$, $\mathcal{G} \subseteq \mathcal{F}$ a $\sigma$-algebra. If $Y$ is bounded and $\mathcal{G}$-measurable, then
	\[
		\mathbb{E}[XY|\mathcal{G}] = \mathbb{E}[X|\mathcal{G}] Y \qquad \text{\normalfont{a.s.}}
	\]
\end{proposition}

\begin{proofbox}
	$Y$ is $\mathcal{G}$-measurable, so $\mathbb{E}[X | \mathcal{G}]Y$ is $\mathcal{G}$-measurable.

	For $A \in \mathcal{G}$,
	\begin{align*}
		\mathbb{E}[(\mathbb{E}[X|\mathcal{G}]Y)\mathbbm{1}_{A}] &= \mathbb{E}[\mathbb{E}[X|\mathcal{G}](Y \mathbbm{1}_{A})] = \mathbb{E}[X Y \mathbbm{1}_{A}].
	\end{align*}
\end{proofbox}

% lecture 4

\begin{definition}
	Let $\mathcal{A}$ be a collection of subsets of $\Omega$. Then $\mathcal{A}$ is a \emph{$\pi$-system}\index{$\pi$-system} if for all $A, B \in \mathcal{A}$, $A \cap B \in \mathcal{A}$, and $\emptyset \in \mathcal{A}$.
\end{definition}

\begin{theorem}
	Let $\mu_1, \mu_2$ be measures on $(E, \mathcal{E})$. Suppose that $\mathcal{A}$ is a $\pi$-system which generates $\mathcal{E}$, and
	\[
	\mu_1(A) = \mu_2(A)
	\]
	for all $A \in \mathcal{A}$, and $\mu_1(E) = \mu_2(E)$ is finite.

	Then $\mu_1 = \mu_2$.
\end{theorem}

\begin{proposition}
	Let $X \in \mathcal{L}^1$, and $\mathcal{G}, \mathcal{H} \subseteq \mathcal{F}$ be $\sigma$-algebras. If $\sigma(X, \mathcal{G})$ are independent of $\mathcal{H}$, then
	\[
		\mathbb{E}[X|\sigma(\mathcal{G}, \mathcal{H})] = \mathbb{E}[X|\mathcal{G}] \qquad \text{\normalfont{a.s.}}
	\]
\end{proposition}

\begin{proofbox}
	Without loss of generality, $X \geq 0$, since the general case comes from decomposing $X = X^+ - X^-$. Let $A \in \mathcal{G}$, $B \in \mathcal{H}$. Then,
	\begin{align*}
		\mathbb{E}[\mathbb{E}[X|\sigma(\mathcal{G},\mathcal{H})]\mathbbm{1}_{A \cap B}] &= \mathbb{E}[X \mathbbm{1}_{A \cap B}] = \mathbb{E}[X \mathbbm{1}_{A}]\mathbb{P}(B) \text{ (independence)} \\
												&= \mathbb{E}[\mathbb{E}[X|\mathcal{G}] \mathbbm{1}_{A}] \mathbb{P}(B) = \mathbb{E}[\mathbb{E}[X|\mathcal{G}] \mathbbm{1}_{A \cap B}].
	\end{align*}
	Define two measures
	\begin{align*}
		\mu_1(F) &= \mathbb{E}[\mathbb{E}[X|\mathcal{G}] \mathbbm{1}_{F}], \\
		\mu_2(F) &= \mathbb{E}[\mathbb{E}[X | \sigma(\mathcal{G}, \mathcal{H})] \mathbbm{1}_{F}].
	\end{align*}
	These are measures on $\mathcal{F}$ which agree on the $\pi$-system $\{A \cap B \mid A \in \mathcal{G}, B \in \mathcal{H}\}$, generating $\sigma(\mathcal{G}, \mathcal{H})$. Moreover since $X \in \mathcal{L}^1$,
	\[
	\mu_1(F) = \mathbb{E}[\mathbb{E}[X|\mathcal{G}]] = \mathbb{E}[X] = \mu_2(F) < \infty.
	\]
	We can now use uniqueness of measures to see these two measures agree on $\sigma(\mathcal{G}, \mathcal{H})$, which can only occur if
	\[
		\mathbb{E}[X|\mathcal{G}] = \mathbb{E}[X|\sigma(\mathcal{G}, \mathcal{H})] \qquad \text{a.s.}
	\]
\end{proofbox}

\subsection{Examples of Conditional Expectation}%
\label{sub:exp}

\begin{exbox}[Gaussians]
Let $(X, Y)$ be a Gaussian random vector in $\mathbb{R}^2$. Our goal is to compute
\[
X' = \mathbb{E}[X|\mathcal{G}],
\]
where $\mathcal{G} = Y$. Since $X'$ is a $\mathcal{G}$-measurable function, there exists a Borel measurable function $f$ so that $X' = f(Y)$. We want to find $f$.

We guess that $X' = aY + b$, for $a, b \in \mathbb{R}$. Then,
\[
a \mathbb{E}[Y] + b = \mathbb{E}[\mathbb{E}[X|\mathcal{G}]] = \mathbb{E}[X],
\]
and moreover
\begin{align*}
	\mathbb{E}[(X - X') Y] &= 0 \\
	\implies \Cov(X - X', Y) &= 0 \\
	\implies \Cov(X, Y) = \Cov(X', Y) &= a \Var(Y).
\end{align*}
We can now take $a$ to satisfy this equation, so $\Cov(X - X', Y) = 0$.

But since $(X - X', Y)$ is a Gaussian random variable with covariance $0$, $X - X'$ and $Y$ are independent.

Suppose that $Z$ is a $\sigma(Y)$-measurable random variable. Then $Z$ is independent of $X - X'$, so
\[
\mathbb{E}[(X - X')Z] = \mathbb{E}[X - X']\mathbb{E}[Z] = 0.
\]
This shows the projection property. Hence
\[
\mathbb{E}[X | \mathcal{G}] = aY + b,
\]
where $a, b$ are determined as before.
\end{exbox}

\begin{exbox}[Conditional Density Functions]
	Suppose that $X, Y$ are random variables with a joint density function $f_{X, Y}$ on $\mathbb{R}^2$. Let $h : \mathbb{R} \to \mathbb{R}$ be Borel measurable so that $h(X)$ is integrable.

	Our goal is to compute
	\[
	\mathbb{E}[h(X) | Y] = \mathbb{E}[h(X) | \sigma(Y)].
	\]
	The density for $Y$ is given by
	\[
	f_Y(y) = \int_{\mathbb{R}} f_{X, Y} (x, y) \diff x.
	\]
	Let $g$ be a bounded, measurable function. Then,
	\begin{align*}
		\mathbb{E}[h(X) g(Y)] &= \int h(x) g(y) f_{X, Y}(x, y) \diff x \diff y \\
				      &= \int \left( \int h(x) \frac{f_{X, Y}(x, y)}{f_{Y}(y)} \diff x \right) g(y) f_{Y}(y) \diff y,
	\end{align*}
	where if $f_Y(y) = 0$, the inner integral is 0. We set
	\[
	\phi(y) =
	\begin{dcases}
		\int h(x) f_{X, Y}(x, y)/f_Y(y) \diff x & \text{if } f_Y(y) > 0,\\
		\;0 & \text{else}.
	\end{dcases}
	\]
	Then,
	\[
		\mathbb{E}[h(X)|Y] = \phi(Y) \qquad \text{a.s.}
	\]
	since $\phi(Y)$ is $\sigma(Y)$-measurable and satisfies the property defining the conditional expectation.

	We interpret this computation as giving that
	\[
	\mathbb{E}[h(X)|Y] = \int_{\mathbb{R}}h(x) \nu(Y, \diff x),
	\]
	where
	\begin{align*}
		\nu(y, \diff x) &= \frac{f_{X, Y}(x, y)}{f_Y(y)} \mathbbm{1}_{f_Y(y) \geq 0} \diff x \\
				&= f_{X|Y}(x|y) \diff x.
	\end{align*}
	$\nu(y, \diff x)$ gives the \emph{conditional distribution}\index{conditional distribution} of $X$ given $Y = y$, and $f_{X|Y}(x|y)$ is the \emph{conditional density function}\index{conditional density function} of $X$ given $Y = y$.
\end{exbox}

In this case, the conditional expectation corresponds to an actual expectation. This corresponds to a regular conditional probability distribution.

Also note $f_{X|Y}(x|y)$ is only defined up to a set of measure $0$.

% lecture 5

\newpage

\section{Discrete Time Martingales}%
\label{sec:dtm}

Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space, and $(E, \mathcal{E})$ a measurable space.

A sequence of random variables $X = (X_n)$ with values in $E$ is a (discrete time) \emph{stochastic process}\index{stochastic process}. A \emph{filtration}\index{filtration} $(\mathcal{F}_n)$ is a sequence of $\sigma$-algebras with $\mathcal{F}_n \subseteq \mathcal{F}_{n+1} \subseteq \mathcal{F}$.

The \emph{natural filtration}\index{natural filtration} $(\mathcal{F}_n^X)$ associated with $X$ is
\[
\mathcal{F}_n^X = \sigma(X_1, \ldots, X_n).
\]
We say that $X$ is \emph{adapted}\index{adapted} to $(\mathcal{F}_n)$ if $X_n$ is $\mathcal{F}_n$-measurable for all $n$. $X$ is always adapted to its natural filtration. Say that $X$ is \emph{integrable}\index{integrable process} if $X_n \in \mathcal{L}^1$ for all $n$.

\begin{definition}
	Let $(\Omega, \mathcal{F}, (\mathcal{F}_n), \mathbb{P})$ be a filtered probability space and let $X$ be a stochastic process which is integrable, adapted and real valued.
	\begin{itemize}
		\item $X$ is a \emph{martingale}\index{martingale} (MG) if
			\[
				\mathbb{E}[X_n| \mathcal{F}_m] = X_m \qquad \text{a.s.}
			\]
			whenever $n \geq m$.
		\item $X$ is a \emph{supermartingale}\index{supermartingale} ($\sup$ MG) if
			\[
				\mathbb{E}[X_n|\mathcal{F}_m] \leq X_m \qquad \text{a.s.}
			\]
			whenever $n \geq m$.
		\item $X$ is a \emph{submartingale}\index{submartingale} (sub MG) if
			\[
				\mathbb{E}[X_n|\mathcal{F}_m] \geq X_m \qquad \text{a.s.}
			\]
			whenever $n \geq m$.
	\end{itemize}
\end{definition}

If $X$ is a MG/sup MG/sub MG, then it is also a MG/sup MG/sub MG with respect to its natural filtration.

\begin{exbox}
	\begin{enumerate}
		\item Say
			\[
			X_n = \sum_{i = 1}^n \xi_i,
			\]
			where $(\xi_i)$ are IID, integrable and $\mathbb{E}[\xi_i] = 0$.
		\item We could also have
			\[
			X_n = \prod_{i = 1}^n \xi_i,
			\]
			where $(\xi_i)$ are IID, integrable and $\mathbb{E}[\xi_i] = 1$.
		\item Choosing
			\[
			X_n = \mathbb{E}[Z | \mathcal{F}_n],
			\]
			where $\mathcal{F} \in \mathcal{L}^1$ and $(\mathcal{F}_n)$ is a filtration, this gives a martingale.
	\end{enumerate}	
\end{exbox}

Martingales are very useful for:
\begin{itemize}
	\item computations (optimal stopping theorem),
	\item bounds (Doob's inequalities),
	\item proving theorems (martingale convergence theorem).
\end{itemize}

\subsection{Stopping Times}%
\label{sub:stop}

\begin{definition}
	Let $(\Omega, \mathcal{F}, (\mathcal{F}_n), \mathbb{P})$ be a filtered probability space. A \emph{stopping time}\index{stopping time} is a random variable $T : \Omega \to \mathbb{Z}_+ \cup \{\infty\}$ with
	\[
		\{T \leq n\} \in \mathcal{F}_n,
	\]
	for all $n$.
\end{definition}

This is equivalent to $\{T = n\} \in \mathcal{F}_n$, for discrete time.

\begin{exbox}
	\begin{enumerate}
		\item Constant (deterministic) times.
		\item First hitting times: if $(X_n)$ is an adapted stochastic process with values in $\mathbb{R}$, and $A \in \mathcal{B}(\mathbb{R})$, then
			\[
			T_A = \inf\{n \geq 0 \mid X_n \in \mathcal{A}\}.
			\]
		\item Last exit times are not always stopping times.
	\end{enumerate}	
\end{exbox}

\begin{proposition}
	Suppose that $S, T, (T_n)$ are stopping times on $(\Omega, \mathcal{F}, (\mathcal{F}_n), \mathbb{P})$. Then:
	\begin{align*}
		&S \wedge T, &  &S \vee T & & \inf T_n, \\
		&\sup T_n, & &\liminf T_n, & & \limsup T_n,
	\end{align*}
	are stopping times
\end{proposition}

\begin{definition}
	Let $T$ be a stopping time on $(\Omega, \mathcal{F}, (\mathcal{F}_n), \mathbb{P})$. Then
	\[
		\mathcal{F}_T = \{A \in \mathcal{F} \mid A \cap \{T \leq n\} \in \mathcal{F}_n \text{ for all }n \}
	\]
	is called the \emph{stopped $\sigma$-algebra}\index{stopped $\sigma$-algebra}.
\end{definition}

For $T = n$ deterministic, $\mathcal{F}_T = \mathcal{F}_n$. For $X$ a stochastic process, $X_T = X_{T(\omega)}(\omega)$, whenever $T(\omega) < \infty$.

The \emph{stopped process}\index{stopped process} $X^T$ is defined by
\[
X^T_n = X_{ n \wedge T}.
\]

\begin{proposition}
	Let $S, T$ be stopping times and $X$ an adapted process. Then,
	\begin{enumerate}[\normalfont(i)]
		\item If $S \leq T$, then $\mathcal{F}_S \subseteq \mathcal{F}_T$.
		\item $X_T \mathbbm{1}_{\{T < \infty\}}$ is $\mathcal{F}_T$-measurable.
		\item $X^T$ is adapted.
		\item If $X$ is integrable, so is $X^T$.
	\end{enumerate}
\end{proposition}

\begin{proofbox}
	
	
	(i) This is immediate from definition.

	(ii) Take $A \in \mathcal{E}$, then
	\begin{align*}
		\{X_T \mathbbm{1}_{\{T < \infty\}} \in A \} \cap \{T \leq n\} = \bigcup_{k = 0}^n (\{X_k \in A\} \cap \{T = k\}) \in \mathcal{F}_n.
	\end{align*}
	Since $X$ is adapted, $\{T = k\} \in \mathcal{F}_k$.

	(iii) For all $n$, $X_{T \wedge n}$ is $\mathcal{F}_{T \wedge n}$-measurable by (ii), so they are $\mathcal{F}_n$-measurable by (i).

	(iv) Note
	\begin{align*}
		\mathbb{E}[|X_{T \wedge n}|] &= \mathbb{E}\left[ \sum_{k = 0}^{n-1} |X_k| \mathbbm{1}(T = k) \right] + \mathbb{E}\left[ \sum_{k = n}^\infty |X_n| \mathbbm{1}(T = k) \right] \\
					     &\leq \sum_{k = 0}^n \mathbb{E}[|X_k|] < \infty.
	\end{align*}
\end{proofbox}

\begin{theorem}[Optional Stopping Theorem]
	Let $(X_n)$ be a MG. Then:
	\begin{enumerate}[\normalfont(i)]
		\item If $T$ is a stopping time, then $X^T$ is a MG, hence
			\[
			\mathbb{E}[X_{T \wedge n}] = \mathbb{E}[X_0]
			\]
			for all $n$.
		\item If $S \leq T$ are bounded stopping times, then
			\[
				\mathbb{E}[X_T|\mathcal{F}_S] = X_S \qquad \text{\normalfont{a.s.}}
			\]
		\item If $S \leq T$ are bounded stopping times, then
			\[
			\mathbb{E}[X_T] = \mathbb{E}[X_S].
			\]
		\item If there exists an integrable random variable $Y$ such that $|X_n| \leq Y$ for all $n$, then for all almost-surely finite stopping times $T$,
			\[
			\mathbb{E}[X_T] = \mathbb{E}[X_0].
			\]
		\item If $X$ has bounded increments, so $|X_{n+1} - X_n| \leq M$, and $T$ is a stopping time with finite expectation, then
			\[
			\mathbb{E}[X_T] = \mathbb{E}[X_0].
			\]
	\end{enumerate}
\end{theorem}

\begin{proofbox}
	

	(i) By the tower property, we only need to show that
	\[
		\mathbb{E}[X_{T \wedge n} | \mathcal{F}_{n-1}] = X_{T \wedge (n-1)} \qquad \text{a.s.}
	\]
	We have
	\begin{align*}
		\mathbb{E}[X_{T \wedge n}|\mathcal{F}_{n-1}] &= \mathbb{E}\left[ \sum_{k = 0}^{n-1} X_k \mathbbm{1}(T = k) \bigm| \mathcal{F}_{n-1} \right] + \mathbb{E}[X_n \mathbbm{1}(T > n-1) | \mathcal{F}_{n-1}] \\
							     &= X_T \mathbbm{1}_{(T < n-1)} + X_{n-1} \mathbbm{1}_{(T > n - 1)} = X_{T \wedge (n - 1)}.
	\end{align*}
	Since $(T > n-1) \in \mathcal{F}_{n-1}$, $\mathbb{E}[X_n|\mathcal{F}_{n-1}] = X_{n-1}$.

	(ii) Suppose that $T \leq n$, and $S \leq T$. Then,
	\begin{align*}
		X_T &= (X_T - X_{T-1}) + \cdots + (X_{S+1} - X_S) + X_S \\
		    &= X_S + \sum_{k = 0}^n (X_{k+1} - X_k) \mathbbm{1}_{S \leq k < T}.
	\end{align*}
	Let $A \in \mathcal{F}_S$. Then,
	\begin{align*}
		\mathbb{E}[X_T \mathbbm{1}_{A}] &= \mathbb{E}[X_S \mathbbm{1}_{A}] + \mathbb{E}\left[ \sum_{k = 0}^n (X_{k+1} - X_k) \mathbbm{1}_{A} \mathbbm{1}(S \leq k < T) \right] \\
						&= \mathbb{E}[X_S \mathbbm{1}_{A}],
	\end{align*}
	since $\{S \leq k < T\} \cap A \in \mathcal{F}_k$ for all $k$, and $X$ is a MG.


	(iii) This follows from taking expectations in (ii).

	(iv) and (v) are on the example sheet.
\end{proofbox}

% lecture 6

\begin{remark}
	\begin{enumerate}
		\item[]
		\item The theorem are holds with supermartingales or submartingales in place of a martingale, with the corresponding inequalities.
		\item The assumptions in the theorem are important. Let $(\xi_k)$ be iid, with $\mathbb{P}(\xi_1 = 1) = \mathbb{P}(\xi_1 = -1) = \frac{1}{2}$, and
			\[
			X_n = \sum_{j = 1}^n \xi_j.
			\]
			Let $T = \inf\{n \geq 0 \mid X_n = 1\}$. Then $T$ is a stopping time with $\mathbb{P}(T < \infty)$. From the optional stopping theorem,
			\[
			\mathbb{E}[X_{T \wedge n}] = \mathbb{E}[X_0] = 0
			\]
			for all $n$, but
			\[
			\mathbb{E}[X_T] = 1 \neq 0.
			\]
			We cannot apply (v) above as $\mathbb{E}[T] = \infty$.
	\end{enumerate}
\end{remark}

\begin{proposition}
	Suppose that $X$ is a non-negative supermartingale. Then for any almost-surely finite stopping time $T$, we have that
	\[
	\mathbb{E}[X_T] \leq \mathbb{E}[X_0].
	\]
\end{proposition}

\begin{proofbox}
	From the optional stopping theorem, $\mathbb{E}[X_{T \wedge n}] \leq \mathbb{E}[X_0]$ for all $n$.

	Apply Fatou's lemma, since $X \geq 0$.
\end{proofbox}

\begin{exbox}[Gambler's Ruin]
	Let $(\xi_k)$ e iid with $\mathbb{P}(\xi_1 = 1) = \mathbb{P}(\xi_1 = -1) = \frac{1}{2}$, and
	\[
	X_n = \sum_{i = 1}^n \xi_i.
	\]
	Let $T_c = \inf\{n \geq 0 \mid X_n = c\}$, and $T = T_{-a} \wedge T_b$ for $a, b > 0$.

	$X$ clearly has bounded increments. We now claim that $\mathbb{E}[T] < \infty$. This is as $T$ is at most the first time that $a + b$ consecutive $\xi_i = 1$.

	Hence the probability of $a+b$ consecutive $+1$'s is $2^{-(a+b)}$, hence
	\[
	T \leq (a + b) \cdot \Geom(2^{-(a+b)}).
	\]
	Hence,
	\[
	\mathbb{E}[T] \leq (a + b) 2^{a + b} < \infty.
	\]
	Hence from the optional stopping theorem,
	\[
	\mathbb{E}[X_T] = \mathbb{E}[X_0] = 0.
	\]
	Also,
	\[
	\mathbb{E}[X_T] = (-a) \mathbb{P}(T_{-a} < T_b) + b \mathbb{P}(T_b < T_{-a}) = 0.
	\]
	We also know that $\mathbb{P}(T_{-a} < T_b) + \mathbb{P}(T_b < T_{-a}) = 1$. Solving this system of equations,
	\[
	\mathbb{P}(T_{-a} < T_b) = \frac{b}{a + b}.
	\]
	
	We can also compute $\mathbb{E}[T]$ using the fact that $X_n^2 - n$ is also a martingale.
\end{exbox}

\subsection{Martingale Convergence Theorem}%
\label{sub:martct}

The goal of this section is to prove the following:

\begin{theorem}[Martingale Convergence Theorem]
	Let $X = (X_n)$ be a supermartingale with
	\[
	\sup_n \mathbb{E}[|X_n|] < \infty.
	\]
	Then $X_n \to X_\infty$ almost surely as $n \to \infty$, where $X_\infty \in \mathcal{L}^1(\mathcal{F}_\infty)$, where
	\[
	\mathcal{F}_\infty = \sigma(\mathcal{F}_n \mid n \geq 0).
	\]
\end{theorem}

\begin{corollary}
	Let $X$ be a non-negative supermartingale. Then $X$ converges almost surely to a finite limit.
\end{corollary}

Let's show this corollary first.

\begin{proofbox}
	Since $X_n \geq 0$,
	\[
	\mathbb{E}[|X_n|] = \mathbb{E}[X_n] \leq \mathbb{E}[X_0] < \infty.
	\]
	So $X$ is $\mathcal{L}^1$ bounded, and we can apply martingale convergence theorem.
\end{proofbox}

Quick detour for some real analysis. Suppose that $x = (x_n)$ is a sequence in $\mathbb{R}$. Fix $a < b$, set $T_0(x) = 0$, and
\begin{align*}
	S_{k+1}(x) &= \inf \{n \geq T_k(x) \mid x_n \leq a\}, \\
	T_{k+1}(x) &= \inf \{n \geq S_{k+1}(x) \mid x_n \geq b\}, \\
	N_n([a, b], x) &= \sup\{k \geq 0 \mid T_k(x) \leq n\},
\end{align*}
the number of up-crossings of $[a, b]$ by $x$ before time $n$. Then,
\[
	N_n([a, b], x) \uparrow N([a, b], x) = \sup\{k \geq 0 \mid T_k < \infty\},
\]
which is the number of up-crossings of $[a, b]$.

\begin{lemma}
	A sequence in $x = (x_n)$ in $\mathbb{R}$ converges in $\mathbb{R} = \mathbb{R} \cup \{-\infty\} \cup \{\infty\}$ if and only if $N([a, b], x) < \infty$ for all $a < b$, $a, b \in \mathbb{Q}$.
\end{lemma}

\begin{proofbox}
	We do the easy direction. Suppose that $x$ converges. Then if there exists $a < b$ with $N([a, b], x) = \infty$, then
	\[
	\liminf_n x_n \leq a, \qquad \limsup_n x_n \geq b,
	\]
	a contradiction to convergence.

	Conversely, if $x$ does not converge, then there exists $a < b$ with $\liminf x_n < a < b < \limsup x_n$, and $a, b \in \mathbb{Q}$. Then
	\[
		N([a, b], x) = \infty.
	\]
\end{proofbox}

One key inequality for the proof is the following:

\begin{theorem}[Doob's Upcrossing Inequality]
	Let $X$ be a supermartingale, and let $a < b$. Then for all $n$,
	\[
		(b - a) \mathbb{E}[N_n([a, b], X)] \leq \mathbb{E}[(X_n - a)^{-}].
	\]
\end{theorem}

\begin{proofbox}
	Let $N = N_n([a, b], X)$. Then,
	\[
		X_{T_k} - X_{S_k} \geq b - a.
	\]
	By definition of $N$,
	\[
	\sum_{k=1}^n (X_{T_{k}{\wedge n}} - X_{S_{k}{ \wedge n}}) = \sum_{k = 1}^N(X_{T_k} - X_{S_k}) + (X_n - X_{S_{N+1}}) \mathbbm{1}_{(S_{N+1} \leq n)}. \tag{$\ast\ast$ }
	\]
	Now $(T_k), (S_k)$ are stopping times, and $S_k \wedge n \leq T_{k} \wedge n$ are bounded stopping times, so from OST,
	\[
	\mathbb{E}[X_{S_{k}\wedge n}] \geq \mathbb{E}[X_{T_{k} {\wedge n}}]
	\]
	for all $k$. Taking the expectation of $(\ast\ast)$,
	\begin{align*}
		0 &\geq \mathbb{E}\left[ \sum_{k = 1}^n (X_{T_k \wedge n} - X_{S_k \wedge n}) \right] \\
		&\geq (b - a)\mathbb{E}[N] - \mathbb{E}[(X_n - a)^{-}],
	\end{align*}
	since from the first equality,
	\[
		(X_n - X_{S_{N+1}})\mathbbm{1}_{S_{N+1} \leq n} \geq - (X_n - a)^{-}.
	\]
\end{proofbox}

Now we prove martingale convergence theorem.

\begin{proofbox}
	Let $a < b$, $a, b \in \mathbb{Q}$. Doob's upcrossing inequality means
	\[
		\mathbb{E}[N_n([a, b], X)]] \leq \frac{1}{b-a} \mathbb{E}[(X_n-a)^{-}].
	\]
	From monotone convergence theorem,
	\[
		\mathbb{E}[N([a, b],X)] \leq \frac{1}{b - a} \sup_n \mathbb{E}[(|X_n| + a)] < \infty.
	\]
	In particular, $N([a, b], X) < \infty$ almost surely, for all $a < b$, $a, b\in \mathbb{Q}$. Hence
	\[
		\Omega_0 = \bigcap_{\substack{a < b\\a, b \in \mathbb{Q}}} \{N([a, b], X) < \infty\} \implies \mathbb{P}(\Omega_0) = 1,
	\]
	since it is an intersection of countably many almost-sure events. Set
	\[
	X_\infty =
	\begin{cases}
		\lim_n X_n & \text{on } \Omega_0,\\
		0 &\text{on } \Omega\setminus \Omega_0.
	\end{cases}
	\]
	Then $X_\infty$ is $\mathcal{F}_\infty$-measurable, and
	\begin{align*}
		\mathbb{E}[|X_\infty|] &= \mathbb{E}[\liminf_n |X_n|] \\
				       &\leq \liminf_n \mathbb{E}[|X_n|] < \infty,
	\end{align*}
	by Fatou's, so $X_\infty \in \mathcal{L}^1$.
\end{proofbox}

% lecture 7

From conditional Jensen's on $|x|$, we for a martingale $X$,
\[
	\mathbb{E}[|X_n| | \mathcal{F}_m] \geq |X_m| \qquad \text{a.s.}
\]
So if $X$ is a martingale, $|X|$ is a non-negative submartingale.

\begin{theorem}[Doob's Maximal Inequality]
	Let $X$ be a non-negative submartingale, and $X_n^\ast = \sup_{0\leq k \leq n} X_k$. Then for all $\lambda \geq 0$,
	\[
		\lambda \mathbb{P}[X_n\ast \geq \lambda] \leq \mathbb{E}[X_n \mathbbm{1}(X_n^\ast \geq \lambda)] \leq \mathbb{E}[X_n].
	\]
\end{theorem}

This can be thought of as a better version of Markov's inequality, that applies to submartingales.

\begin{proofbox}
	Let $T = \inf\{k \geq 0 \mid X_k \geq \lambda\}$. Then $T \wedge n$ is a bounded stopping time. By optional stopping theorem,
	\begin{align*}
		\mathbb{E}[X_n] &\geq \mathbb{E}[X_{T \wedge n}] = \mathbb{E}[X_T \mathbbm{1}_{T \leq n}] + \mathbb{E}[X_n \mathbbm{1}_{T > n}] \\
				&\geq \lambda \mathbb{P}(T \leq n) + \mathbb{E}[X_n \mathbbm{1}_{T > n}].
	\end{align*}
	Note that $\{T \leq n\} = \{X_n^\ast \geq \lambda\}$, so rearranging,
	\[
	\lambda \mathbb{P}(X_n^\ast \geq \lambda) \leq \mathbb{E}[X_n \mathbbm{1}_{T \leq n}] \leq \mathbb{E}[X_n].
	\]
\end{proofbox}

\begin{theorem}[Doob's $\mathcal{L}^p$ Inequality]
	Let $X$ be a non-negative submartingale. For $p > 1$ and with $X_n^\ast = \sup_{0 \leq k \leq n} X^k$, we have that
	\[
	\|X_n^\ast\|_p \leq \frac{p}{p-1} \|X_n\|_p.
	\]
\end{theorem}

\begin{proofbox}
	Fix $k < \infty$. We have that
	\begin{align*}
		\mathbb{E}[(X_n^\ast \wedge k)^p] &= \mathbb{E}\left[ \int_0^k p x^{p-1} \mathbbm{1}(X_n^\ast \geq x) \diff x \right] & & \\
						  &= \int_0^k px^{p-1} \mathbb{P}(X_n^\ast \geq x) \diff x & &\text{(Fubini's)} \\
						  &\leq \int_0^k p x^{p-2} \mathbb{E}[X_n \mathbbm{1}(X_n^\ast > x)] \diff x & & \\
						  &= \frac{p}{p-1} \mathbb{E}[X_n(X_n^\ast \wedge k)^{p-1}] & &\text{(Fubini's)} \\
						  &\leq \frac{p}{p-1} \|X_n\|_p \|X_n^\ast \wedge k\|_p^{p-1} & &\text{(H\"older's)}.
	\end{align*}
	This gives us
	\[
	\|X_n^\ast \wedge k\|_p \leq \frac{p}{p-1} \|X_n\|_p.
	\]
	Applying MCT, we can send $k \to \infty$, and get
	\[
	\|X_n^\ast\|_ \leq \frac{p}{p-1}\|X_n\|_p.
	\]
\end{proofbox}

\subsection{\texorpdfstring{$\mathcal{L}^p$}{Lp} Convergence of Martingales}%
\label{sub:lp_conv_mg}

\begin{theorem}
	Let $X$ be a martingale, and $p > 1$. The following are equivalent:
	\begin{enumerate}[\normalfont(i)]
		\item $X$ is $\mathcal{L}^p$ bounded:
			\[
			\sup_{n \geq 0} \|X_n\|_p < \infty.
			\]
		\item $X$ converges almost-surely and in $\mathcal{L}^p$ to $X_\infty$.
		\item There exists $Z \in \mathcal{L}^p$ such that $X_n = \mathbb{E}[Z|\mathcal{F}_n]$ almost-surely, for all $n$.
	\end{enumerate}
\end{theorem}

\begin{proofbox}
	

	(i) $\implies$ (ii): Suppose that $X$ is $\mathcal{L}^p$ bounded. Then from Jensen, $X$ is also $\mathcal{L}^1$. So martingale convergence theorem gives $X_n \to X_\infty$ almost-surely. Then,
	\[
	\mathbb{E}[|X_\infty|^p] = \mathbb{E}[\liminf_n |X_n|^p] \leq \liminf_n \mathbb{E}[|X_n|^p] \leq \sup_n \|X_n\|_p^p < \infty.
	\]
	Doob's $\mathcal{L}^p$ inequality gives
	\[
	\|X_n^\ast\|_p \leq \frac{p}{p-1} \|X_n\|_p,
	\]
	where $X_n^\ast = \sup_{0 \leq k \leq n} |X_n|$. From monotone convergence theorem,
	\[
	\|X_\infty^\ast\|_p \leq \frac{p}{p-1} \sup_n \|X_n\|_p < \infty.
	\]
	Thus, $|X_n - X_\infty| \leq 2 X_\infty^\ast \in \mathcal{L}^p$. So from dominated convergence theorem, $X_n \to X_\infty$ in $\mathcal{L}^p$.

	(ii) $\implies$ (iii): Let $Z = X_\infty$, where $Z \in \mathcal{L}^p$. We will show that $X_n = \mathbb{E}[Z|\mathcal{F}_n]$. Now, for any $m \geq n$,
	\begin{align*}
		\|X_n - \mathbb{E}[X_\infty|\mathcal{F}_n]\|_p &= \|\mathbb{E}[X_m - X_\infty | \mathcal{F}_n]\|_p \\
							       &\leq \|X_m - X_\infty\|_p \to 0,
	\end{align*}
	hence $X_n = \mathbb{E}[X_\infty | \mathcal{F}_n]$ almost-surel.


	(iii) $\implies$ (i): Let $X_n = \mathbb{E}[Z | \mathcal{F}_n]$, then
	\[
	\|X_n\|_p \leq \|Z\|_p < \infty,
	\]
	so these are $\mathcal{L}^p$-bounded.
\end{proofbox}

\begin{definition}
	A martingale of the form $\mathbb{E}[Z | \mathcal{F}_n]$ for $Z \in \mathcal{L}^p$ is \emph{closed}\index{closed martingale} in $\mathcal{L}^p$.
\end{definition}

\begin{corollary}
	If $Z \in \mathcal{L}^p$, $X_n = \mathbb{E}[Z | \mathcal{F}_n]$, then if we let $\mathcal{F}_\infty = \sigma(\mathcal{F}_n, n \geq 0)$, we have that
	\[
		X_n \to X_\infty = \mathbb{E}[Z | \mathcal{F}_\infty] \qquad \text{\normalfont{a.s}}
	\]
	as $n \to \infty$, and also in $\mathcal{L}^p$.
\end{corollary}

\begin{proofbox}
	From the theorem, $X_n \to X_\infty$ almost-surely, and in $\mathcal{L}^p$. What we want to show is that $X_\infty = \mathbb{E}[Z | \mathcal{F}_\infty]$.

	Clearly $X_\infty$ is $\mathcal{F}_\infty$-measurable. Fix $A \in \bigcup \mathcal{F}_n$, then $A \in \mathcal{F}_N$ for some $N$. For $n \geq N$,
	\[
	\mathbb{E}[Z \mathbbm{1}_{A}] = \mathbb{E}[X_n \mathbbm{1}_{A}] \to \mathbb{E}[X_\infty \mathbbm{1}_{A}],
	\]
	hence
	\[
	\mathbb{E}[X_\infty \mathbbm{1}_{A}] = \mathbb{E}[\mathbb{E}[Z | \mathcal{F}_\infty] \mathbbm{1}_{A}].
	\]
	Note $\bigcup \mathcal{F}_n$ is a $\pi$-system which generates $\mathcal{F}_\infty$, so this holds for all $A$. Hence this gives $\mathbb{E}[Z|\mathcal{F}_\infty] = X_\infty$.
\end{proofbox}

\subsection{Uniformly Integrable Martingales}%
\label{sub:ui_mg}

\begin{definition}
	A collection of random variables $(X_i)_{i \in I}$ is called \emph{uniformly integrable}\index{uniformly integrable} (UI) if
	\[
		\sup_{i \in I} \mathbb{E}[|X_i| \mathbbm{1}(|X_i| > \alpha)] \to 0
	\]
	as $\alpha \to \infty$. Alternatively, $(X_i)$ is UI if $(X_i)$ is $\mathcal{L}^1$-bounded, and for all $\eps > 0$, there exists $\delta > 0$ such that if $\mathbb{P}(A) < \delta$, then
	\[
	\sup_{i \in I} \mathbb{E}[|X_i| \mathbbm{1}_{A}] < \eps.
	\]
\end{definition}

If $(X_i)$ is $\mathcal{L}^p$ bounded for some $p > 1$, then the family is UI. If $(X_i)$ is UI, then the family is $\mathcal{L}^1$-bounded. However the converse is not true.

\begin{theorem}
	Let $X \in \mathcal{L}^1$. Then
	\[
		\{\mathbb{E}[X | \mathcal{G}] \mid G \subseteq \mathcal{F} \text{ a $\sigma$-algebra}\}
	\]
	is UI.
\end{theorem}

\begin{proofbox}
	Since $X \in \mathcal{L}^1$, for all $\eps > 0$ there exists $\delta > 0$ such that
	\[
	\mathbb{P}(A) < \delta \implies \mathbb{E}[|X| \mathbbm{1}_{A}] < \eps.
	\]
	Choose $\lambda > 0$ such that $\mathbb{E}[|X|] \leq \lambda \cdot \delta$. Suppose that $\mathcal{G} \subseteq \mathcal{F}$, and let $Y = \mathbb{E}[X|\mathcal{G}]$. We know that $\mathbb{E}[|Y|] < \mathbb{E}[|X|] < \infty$.

	Applying Markov's inequality,
	\[
	\mathbb{P}(|Y| \geq \lambda) \leq \frac{\mathbb{E}[|X|]}{\lambda} \leq \delta,
	\]
	so
	\[
	\mathbb{E}[|Y| \mathbbm{1}(|Y| > \lambda)] \leq \mathbb{E}[|X| \mathbbm{1}(|Y| \geq \lambda)] \leq \eps,
	\]
	by conditional Markov's.
\end{proofbox}

% lecture 8

\begin{lemma}
	Let $(X_n)$, $X$ be $\mathcal{L}^1$ with $X_n \to X$ almost surely, then
	\[
		X_n \to X \text{ in } \mathcal{L^1} \iff (X_n) \text{ uniformly integrable}.
	\]
\end{lemma}

\begin{definition}
	A martingale $X = (X_n)$ is called a \emph{uniformly integrable martingale}\index{uniformly integrable martingale} if and only if $X$ is a martingale, and $(X_n)$ is a uniformly integrable family.
\end{definition}

\begin{theorem}
	Let $X$ be a martingale. The following are equivalent:
	\begin{enumerate}[\normalfont(i)]
		\item $X$ is a UI martingale.
		\item $X_n$ converges almost surely and in $\mathcal{L}^1$ to $X_\infty$.
		\item There exists $Z \in \mathcal{L}^1$ such that $X_n = \mathbb{E}[Z | \mathcal{F}_n]$ almost surely, for all $n$
	\end{enumerate}
\end{theorem}

\begin{proofbox}
	

	(i) $\implies$ (ii): Since $X$ is UI, it is $\mathcal{L}^1$ bounded, so $X_n \to X_\infty$ almost-surely, from martingale convergence theorem. Hence also $X_n \to X_\infty$ in $\mathcal{L}^1$, as $X$ is uniformly integrable.

	(ii) $\implies$ (iii): Set $Z = X_\infty$, so $Z \in \mathcal{L}^1$. We want to show that
	\[
		X_n = \mathbb{E}[Z | \mathcal{F}_n] \qquad \text{a.s.}
	\]
	From the martingale property,
	\begin{align*}
		\|X_n - \mathbb{E}[X_\infty | \mathcal{F}_n]\|_1 &= \|\mathbb{E}[X_m -X_\infty | \mathcal{F}_n]\| \leq \|X_m - X_\infty\| \\
								 &\to 0,
	\end{align*}
	so $X_n = \mathbb{E}[X_\infty | \mathcal{F}_n]$.

	(iii) $\implies$ (i): From the tower property, $\mathbb{E}[Z|\mathcal{F}_n]$ is a martingale, which is uniformly integrable from the previous theorem.
\end{proofbox}

\begin{remark}
	\begin{enumerate}
		\item[]
		\item As before, $X$ is a UI martingale $\implies$ $\mathbb{E}[Z | \mathcal{F}_\infty] = X_\infty$.
		\item If $X$ is a UI sub MG or sup MG, then $X_n \to X_\infty$ almost surely, and in $\mathcal{L}^1$, where $\mathbb{E}[X_\infty | \mathcal{F}_n] \leq X_n$ for a submartingale.
		\item Let $(X_i)$ be iid with $\mathbb{P}(X_1 = 0) = \mathbb{P}(X_1 = 2) = \frac{1}{2}$. Then
			\[
			Y_n = \prod_{j = 1}^n X_j
			\]
			is a martingale which is $\mathcal{L}^1$ bounded and converges to $0$ almost-surely. But $\mathbb{E}[Y_n] = 1$ for all $n$, so $Y_n \not \to Y_\infty$ in $\mathcal{L}^1$.
	\end{enumerate}
\end{remark}

For $X$ a UI martingale, and $T$ a stopping time, then we can consider
\[
X_T = \sum_{n = 0}^{\infty} X_n \mathbbm{1}(T=n) + X_\infty \mathbbm{1}(T = \infty).
\]
\begin{theorem}[Optional Stopping Theorem for UI Martingales]
	Let $X$ be a UI martingale, and $S, T$ stopping times with $S \leq T$. Then,
	\[
		\mathbb{E}[X_T | \mathcal{F}_S] = X_s \qquad\text{\normalfont{a.s.}}
	\]
\end{theorem}

\begin{proofbox}
	We claim that
	\[
		\mathbb{E}[X_\infty | \mathcal{F}_T] = X_T \qquad \text{a.s.}
	\]
	for all stopping times $T$. First we show that $X_T \in \mathcal{L}^1$. Note that
	\[
	|X_n| \leq \mathbb{E}[|X_\infty| | \mathcal{F}_n],
	\]
	so
	\begin{align*}
		\mathbb{E}[|X_T|] &= \sum_{n = 0}^{\infty} \mathbb{E}[|X_n| \mathbbm{1}(T=n)] + \mathbb{E}[|X_\infty| \mathbbm{1}(T = \infty)] \\
				  &\leq \sum_{n = 0}^{\infty} \mathbb{E}[|X_\infty| \mathbbm{1}(T=n)] = \mathbb{E}[|X_\infty|].
	\end{align*}
	Let $B \in \mathcal{F}_T$. Then
	\begin{align*}
		\mathbb{E}[X_T \mathbbm{1}_{B}] &= \sum_n \mathbb{E}[X_n \mathbbm{1}(T = n) \mathbbm{1}_{B}] \\
						&= \sum_n \mathbb{E}[X_\infty \mathbbm{1}(T = n) \mathbbm{1}_{B}] \\
						&= \mathbb{E}[X_\infty \mathbbm{1}_{B}].
	\end{align*}
	Since $X_T$ is $\mathcal{F}_T$-measurable, this implies $\mathbb{E}[X_\infty | \mathcal{F}_T] = X_T$ almost-surely.

	Suppose that $S \leq T$ are stopping times. Then from the tower property,
	\begin{align*}
		\mathbb{E}[X_T |\mathcal{F}_S] &= \mathbb{E}[ \mathbb{E}[X_\infty | \mathcal{F}_T] | \mathcal{F}_s] \\
					       &= \mathbb{E}[X_\infty | \mathcal{F}_S] = X_s \qquad \text{a.s.}
	\end{align*}
\end{proofbox}

\subsection{Backwards Martingales}%
\label{sub:bms}

Let
\[
\cdots \subseteq \mathcal{G}_{-2} \subseteq \mathcal{G}_{-1} \subseteq \mathcal{G}_0
\]
be a sequence of $\sigma$-algebras indexed by $\mathbb{Z}_-$. Sa that $X = (X_n)_{n \leq 0}$ is a \emph{backwards martingale}\index{backwards martingale} if it is adapted, $X_n \in \mathcal{L}^1$, and
\[
	\mathbb{E}[X_{n+1} | \mathcal{G}_n] = X_n \qquad \text{a.s.}
\]
From the tower property, we have
\[
	\mathbb{E}[X_0 | \mathcal{G}_n] = X_n \qquad \text{a.s.}
\]
for all $n \leq 0$. Since $X_0 \in \mathcal{L^1}$, $X$ is UI.

\begin{theorem}
	Let $X$ be a backwards martingale, $X_0 \in \mathcal{L}^p$, for $p \geq 1$. Then $X_n$ converges almost-surely and in $\mathcal{L}^p$ as $n \to -\infty$ to
	\[
	X_{-\infty} = \mathbb{E}[X_0 | \mathcal{G}_{-\infty}],
	\]
	where
	\[
	\mathcal{G}_{-\infty} = \bigcap_{n \leq 0} \mathcal{G}_n.
	\]
\end{theorem}

\begin{proofbox}
	We have multiple steps.

	First we use Doob's upcrossing inequality for $a < b$. Let $N_{-n}([a, b], X)$ be the number of upcrossings of $X$ across $[a, b]$ in the time interval from $-n$ to $0$, and let
	\[
	\mathcal{F}_k = \mathcal{G}_{-n + k},
	\]
	for $0 \leq k \leq n$. Then:
	\begin{itemize}
		\item $(\mathcal{F}_k)$ is increasing.
		\item $(X_{-n+k})$ is a martingale.
	\end{itemize}
	So from Doob,
	\[
		(b - a) \mathbb{E}[N_{-n}([a, b], X)] \leq \mathbb{E}[(X_0 - a)^{-}].
	\]
	As $n \to \infty$, $N_{-n}([a, b], X)$ goes to the number of upcrossings of $X$ across $[a, b]$. So by monotone convergence theorem,
	\[
		\mathbb{E}[N_{-\infty}([a, b], X)] < \infty.
	\]
	Hence $X_m \to X_{-\infty}$ as $m \to -\infty$ almost-surely.

	Next we need to argue $\mathcal{L}^p$ convergence. If $X_0 \in \mathcal{L}^p$, then $X_n \in \mathcal{L}^p$ for all $n \leq 0$, by conditional Jensen's.

	Hence by Fatou, $X_{-\infty} \in \mathcal{L}^p$. So to show $\mathcal{L}^p$ convergence, we need
	\begin{align*}
		|X_n - X_{-\infty}|^{p} &= |\mathbb{E}[X_0 - X_{-\infty} | \mathcal{G}_n]|^p \leq \mathbb{E}[|X_0 - X_{-\infty}|^p | \mathcal{F}_n].
	\end{align*}
	However the item on the right is uniformly integrable, as it is a random variable conditioned on different $\sigma$-algebras. Hence $|X_n - X_{-\infty}|^p$ is a UI family that converges to 0 a.s., hence converges to $0$ in $\mathcal{L}^1$. So $X_n \to X_{-\infty}$ in $\mathcal{L}^p$.

	The final thing to show is that
	\[
		X_{-\infty} = \mathbb{E}[X_0 | \mathcal{G}_{-\infty}] \qquad \text{a.s.}
	\]
	We need to show that for all $A \in \mathcal{G}_{-\infty}$, that $\mathbb{E}[X_0 \mathbbm{1}_{A}] = \mathbb{E}[X_{-\infty} \mathbbm{1}_{A}]$.

	As $A \in \mathcal{G}_{-n}$ for all $n$, by properties of martingales
	\[
	\mathbb{E}[X_0 \mathbbm{1}_{A}] = \mathbb{E}[X_n \mathbbm{1}_{A}].
	\]
	As $n \to -\infty$, from $\mathcal{L}^1$ convergence we get
	\[
	\mathbb{E}[X_0 \mathbbm{1}_{A}] = \mathbb{E}[X_{-\infty} \mathbbm{1}_{A}].
	\]
\end{proofbox}

\subsection{Applications of Martingale Theory}%
\label{sub:app_mart}

Here are some nice applications.

\begin{theorem}[Kolmogorov 0-1 Law]
	Let $(X_i)$ be IID random variables. Let $\mathcal{F}_n = \sigma(X_k \mid k \geq n)$, and
	\[
	\mathcal{F}_\infty = \bigcap_{n \geq 0} \mathcal{F}_n.
	\]
	Then $\mathcal{F}_\infty$ is trivial, i.e. for all $A \in \mathcal{F}_\infty$, $\mathbb{P}(A) \in \{0, 1\}$.
\end{theorem}

\begin{proofbox}
	Let $\mathcal{G}_n = \sigma(X_k \mid k \leq n)$, and $A \in \mathcal{F}_\infty$.

	Since $\mathcal{G}_n$ is independent of $\mathcal{F}_{n+1}$, we have
	\[
		\mathbb{E}[\mathbbm{1}_{A} | \mathcal{G}_n] = \mathbb{P}(A) \qquad \text{a.s.}
	\]
	From martingale convergence theorem,
	\[
		\mathbb{E}[\mathbbm{1}_{A} | \mathcal{G}_n] \to \mathbb{E}[\mathbbm{1}_{A} | \mathcal{G}_\infty] \qquad \text{a.s.}
	\]
	where $G_\infty = \sigma(G_n \mid n \geq 0)$. So
	\[
		\mathbb{E}[\mathbbm{1}_{A} | \mathcal{G}_\infty] = \mathbbm{1}_{A} = \mathbb{P}(A)\qquad\text{a.s.}
	\]
	since $F_\infty \subseteq G_{\infty}$. This gives $\mathbb{P}(A) \in \{0, 1\}$.
\end{proofbox}

% lecture 9

\begin{theorem}[Strong Law of Large Numbers]
	Let $(X_i)$ be IID in $\mathcal{L}^1$, and let $\mu = \mathbb{E}[X_1]$. Let $S_n = X_1 + X_2 + \cdots + X_n$ 

	Then $S_n/n \to \mu$ almost-surely, and in $\mathcal{L}^1$.
\end{theorem}

\begin{proofbox}
	Let 
	\[
	\mathcal{G}_n = \sigma(S_n, S_{n+1}, S_{n+2}, \ldots) = \sigma(S_n, X_{n+1}, X_{n+2}, \ldots).
	\]
	We claim that
	\[
		(M_n)_{n \leq -1} = \frac{S_{-n}}{-n}
	\]
	is a $(\mathcal{F}_n) = (\mathcal{G}_{-n})$ backwards martingale.

	Indeed, fix $m \leq -1$, then
	\begin{align*}
		\mathbb{E}[M_{m + 1} | \mathcal{F}_m] &= \mathbb{E}\left[ \frac{S_{-m-1}}{-m-1} | \mathcal{G}_{-m} \right] = \mathbb{E}\left[ \frac{S_{n-1}}{n-1} | \mathcal{G}_n \right] \\
						    &= \mathbb{E}\left[ \frac{S_n - X_n}{n-1} | \mathcal{G}_n \right] = \frac{S_n}{n-1} - \mathbb{E}\left[ \frac{X_n}{n-1} | \mathcal{G}_n \right].
	\end{align*}
	Since $X_n$ is independent of $(X_{n+1}, \ldots)$, we have
	\[
	\mathbb{E}\left[ \frac{X_n}{n-1} | \mathcal{G}_n \right] = \frac{1}{n-1} \mathbb{E}[X_n | S_n].
	\]
	By symmetry, $\mathbb{E}[X_k | S_n] = \mathbb{E}[X_1 | S_n]$ for all $1 \leq k \leq n$. Also, $\mathbb{E}[S_n | S_n] = S_n$, so since
	\[
	\mathbb{E}[X_1 | S_n] + \mathbb{E}[X_2 | S_n] + \cdots + \mathbb{E}[X_n | S_n] = S_n \implies \mathbb{E}[X_n | S_n] = S_n/n.
	\]
	Putting this into the formula,
	\begin{align*}
		\mathbb{E}\left[\frac{S_{n-1}}{n-1} | \mathcal{G}_n \right] &= \frac{S_n}{n-1} - \frac{S_n}{n(n-1)} = \frac{S_n}{n},
	\end{align*}
	hence $(S_n/n)$ is a backwards martingale.

	By the backward martingale convergence theorem, $S_n/n \to Y$ almost-surely and in $\mathcal{L}^1$. Since
	\[
	Y = \lim_{n \to \infty} \frac{S_n}{n} = \lim_{n \to \infty} \frac{X_{k+1} + \cdots + X_{k+n}}{n}
	\]
	almost-surely, we have that $Y$ is measurable with respect to $\mathcal{T}_k = \sigma(X_{k+1}, X_{k+2}, \ldots)$, hence it is measurable with respect to
	\[
	\bigcap_{k} \mathcal{T}_k.
	\]
	From the Kolmogorov 0-1 law, this means that $\mathbb{P}(Y = c) = 1$. Then
	\[
	c = \mathbb{E}[Y] = \mathbb{E}[\lim_{n \to \infty} S_n/n] = \lim_{n} \mathbb{E}[S_n/n] = \mu,
	\]
	by $\mathcal{L}^1$ convergence.
\end{proofbox}

\begin{theorem}[Radon-Nikodym Theorem]
	Let $\mathbb{P}, \mathbb{Q}$ be probability measure on $(\Omega, \mathcal{F})$. Assume that $\mathcal{F}$ is countably generated, i.e.
	\[
	\mathcal{F} = \sigma(F_n),
	\]
	where $(F_n)$ is countable. Then the follow are equivalent:
	\begin{enumerate}[\normalfont(i)]
		\item $\mathbb{P}(A) = 0 \implies Q(A) = 0$, for all $A \in \mathcal{F}$, i.e. $\mathbb{Q} \ll \mathbb{P}$.
		\item For all $\eps > 0$, there exists $\delta > 0$ such that for all $A \in \mathcal{F}$,
			\[
			\mathbb{P}(A) < \delta \implies \mathbb{Q}(A) < \eps.
			\]
		\item There exists $X \geq 0$ so that
			\[
			\mathbb{Q}(A) = \mathbb{E}[X \mathbbm{1}_{A}]
			\]
			for all $A \in \mathcal{F}$, where the expectation is taking with respect to $\mathbb{P}$.
	\end{enumerate}	
\end{theorem}

The random variable $X$ is called the \emph{Radon-Nikodym derivative}\index{Radon-Nikodym derivative}, and is usually written $X = \diff \mathbb{Q} / \diff \mathbb{P}$.

This can be extended to finite or $\sigma$-finite measures, and the assumption that $\mathcal{F}$ is countably generated can be removed.

\begin{proofbox}
	

	(i) $\implies$ (ii): Suppose that (ii) does not hold, then there exists $\eps > 0$ such that for all $n$, there is $A_n$ with $\mathbb{P}(A_n) < 1/n^2$ and $\mathbb{Q}(A_n) \geq \eps$.

	By Borel-Cantelli, $\mathbb{P}(A_n \text{ i.o.}) = 0$, so $\mathbb{Q}(A_n \text{ i.o.}) = 0$. But,
	\begin{align*}
		\mathbb{Q}(A_n \text{ i.o.}) &= \mathbb{Q} \left( \bigcap_n \bigcup_{k \geq n} A_k \right) = \lim_n \mathbb{Q} \left( \bigcup_{k \geq n}  A_k \right) \geq \eps,
	\end{align*}
	which is a contradiction.

	(ii) $\implies$ (iii): Let $\mathcal{F}_n = \sigma(F_k \mid k \leq n)$. If we write
	\[
		\mathcal{A}_{n} = \{H_1 \cap \cdots \cap H_N \mid H_i \in \{F_i, F_i^{c}\}\},
	\]
	then $\mathcal{F}_n = \sigma(\mathcal{A}_n)$. Note that sets in $\mathcal{A}_n$ are disjoint. Let
	\[
	X_n(\omega) = \sum_{A \in \mathcal{A}_n} \frac{\mathbb{Q}(A)}{\mathbb{P}(A)} \mathbbm{1}_{A}(\omega).
	\]
	Since the sets in $\mathcal{A}_n$ are disjoint, we have that
	\[
	\mathbb{Q}(A) = \mathbb{E}[X_n \mathbbm{1}_{A}],
	\]
	for all $A \in \mathcal{F}_n$. Write $X_n = \diff \mathbb{Q} / \diff \mathbb{P}$ on $\mathcal{F}_n$. Note that $X_n \geq 0$.

	Moreover, $(X_n)$ is a martingale. Indeed, if $A \in \mathcal{F}_n$, then
	\[
	\mathbb{E}[X_{n+1} \mathbbm{1}_{A}] = \mathbb{Q}(A) = \mathbb{E}[X_n \mathbbm{1}_{A}],
	\]
	hence $\mathbb{E}[X_{n+1} | \mathcal{F}_n] = X_n$. Moreover $(X_n)$ is $\mathcal{L}^1$ bounded as $\mathbb{E}[X_n] = \mathbb{Q}(\Omega) = 1$.

	So by martingale convergence theorem, $X_n \to X_\infty$ almost-surely as $n \to \infty$. We are also going to show that $(X_n)$ is UI. 

	Fix $\eps > 0$. Then there is $\delta > 0$ so that condition (ii) holds. Let $\lambda = 1/\eps$. Then,
	\[
	\mathbb{P}(X_n \geq \lambda) \leq \frac{\mathbb{E}[X_n]}{\lambda} = \frac{1}{\lambda} = \delta.
	\]
	So from (ii),
	\[
	\mathbb{E}[X_n \mathbbm{1}(X_n > \lambda)] = \mathbb{Q}(X_n > \lambda) \leq \eps.
	\]
	Hence $(X_n)$ is UI, and $X_n \to X_\infty$ in $\mathcal{L}^1$, with $\mathbb{E}[X_\infty] = 1$.

	Now we show that $X_\infty$ is the required derivative. Indeed, for all $A \in \mathcal{F}_n$, $\mathbb{E}[X_n \mathbbm{1}_{A}] = \mathbb{E}[X_\infty \mathbbm{1}_{A}]$. Now set
	\[
		\tilde{\mathbb{Q}}(A) = \mathbb{E}[X_\infty \mathbbm{1}_{A}],
	\]
	then $\mathbb{Q} = \tilde{\mathbb{Q}}$ on $\bigcup \mathcal{F}_n$, which is a generating $\pi$-system. Hence $\mathbb{Q} = \tilde{\mathbb{Q}}$.

	(iii) $\implies$ (i): Obvious.
\end{proofbox}

\newpage

\section{Continuous Time Processes}%
\label{sec:ctp}

Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space. We want to define stochastic process indexed by $t \geq 0$, rather than $n \in \mathbb{N}$.

A \emph{filtration}\index{filtration} $(\mathcal{F}_t)_{t \geq 0}$ is an increasing sequence of $\sigma$-algebras, so if $s \leq t$, then $\mathcal{F}_s \subseteq \mathcal{F}_t$.

A collection of random variables $(X_t)$ is called a \emph{stochastic process}\index{stochastic process}. We say that $(X_t)$ is \emph{adapted}\index{adapted} if $X_t$ is $\mathcal{F}_t$-measurable for all $t \geq 0$.

A stopping time $T$ is a random variable with values in $[0, \infty]$ with
\[
	\{T \leq t\} \in \mathcal{F}_t.
\]
Why is continuous time so different? There are many different measurability issues that arise.

In particular, in discrete time we can equip $\mathbb{N}$ with the $\sigma$-algebra $\mathcal{P}(\mathbb{N})$, and $(\omega, n) \mapsto X_n(\omega)$ is measurable with respect to $\mathcal{F} \otimes \mathcal{P}(\mathbb{N})$.

For continuous time, then for fixed $t \geq 0$ the map $\omega \mapsto X_t(\omega)$ is a random variable, but it need not be that $(\omega, t) \mapsto X_t(\omega)$ is measurable with respect to $\mathcal{F} \otimes \mathcal{B}(\mathbb{R})$. An example is
\[
X_t = \mathbbm{1}_{A}(t),
\]
where $A \subseteq \mathbb{R}$ is not Borel.

For $A \subseteq \mathbb{R}$, this means that
\[
	T_A = \inf\{t \geq 0 \mid X_t \in A\}
\]
need not be a stopping time. This is as
\[
	\{T_A \leq t\} = \bigcup_{0 \leq s \leq t} \{T_A = s\} \not \in \mathcal{F}_t
\]
% lecture 10
in general, since we have an uncountable union.

How do we fix these problems? We can:
\begin{itemize}
	\item Require that $t \mapsto X_t (\omega)$ is continuous as this implies that $(\omega, t) \mapsto X_t(\omega)$ is measurable.
	\item Require that $t \mapsto X_t(\omega)$ is a \emph{cadlag}\index{cadlag} (right continuous, and left hand limits exist). Also written as RCLL. So,
		\[
			X_t = \lim_{s \downarrow t} X_s, \qquad \lim_{s \uparrow t} X_s \text{ exists}.
		\]
\end{itemize}

In both cases, the process is determined by its values at a countable set of time, i.e. $\mathbb{Q}_+$.

To see that $t \mapsto X_t(\omega)$ being continuous implies $(\omega, t) \mapsto X_t(\omega)$ is measurable, we first restrict to $t \in [0, 1]$. Now, note that
\[
X_t(\omega) = \lim_{n \to \infty} \sum_{k = 0}^{2^{n}-1} \mathbbm{1}_{(t \in (k 2^{-n}, (k+1)2^{-n}]} X_{k 2^{-n}}(\omega)
\]
is $\mathcal{F} \otimes \mathcal{B}([0, 1])$-measurable, as it is a pointwise limit of measurable function.

We will define a few classes of random variables. Let
\[
	C(\mathbb{R}_+, E) = \{\text{continuous } x : \mathbb{R}_+ \to E\}
\]
with the smallest $\sigma$-algebra which makes $\Pi_t : X \to X_t$ measurable for all $t$, and let
\[
	D(\mathbb{R}_+, E) = \{\text{cadlag functions } x : \mathbb{R}_+ \to E\}
\]
with the same $\sigma$-algebra.

For a stopping time its $\sigma $-algebra is still
\[
	\mathcal{F}_T = \{A \in \mathcal{F} \mid A \cap \{T \leq t\} \in \mathcal{F}_t \text{ for all }t \}.
\]
For $X$ cadlag, set
\[
X_T(\omega) = X_{T(\omega)}(\omega)
\]
whenever $T(\omega) < \infty$. Set $X^{T}_t = X_{t \wedge T}$.

\begin{proposition}
	Let $S, T$ be stopping times, and $X$ a cadlag, adapted process. Then,
	\begin{enumerate}[\normalfont(i)]
		\item $S \wedge T$ is a stopping time.
		\item $S \leq T$ implies $\mathcal{F}_S \subseteq \mathcal{F}_T$.
		\item $X_T \mathbbm{1}_{(T < \infty)}$ is $\mathcal{F}_T$-measurable.
		\item $X^T$ is adapted.
	\end{enumerate}
\end{proposition}

\begin{proofbox}
	

	(i) and (ii) follow from the definition, and (iv) follows from (iii): Note that $X_{T \wedge t}$ is $\mathcal{F}_{T \wedge t}$-measurable from (iii), and $\mathcal{F}_{T \wedge t} \subseteq \mathcal{F}_t$.

	So we are only required to show (iii). To do this, we claim the following: $Z$ is $\mathcal{F}_T$-measurable if and only if $Z \mathbbm{1}(T \leq t)$ is $\mathcal{F}_t$-measurable for all $t$.

	In the forwards direction, if $Z$ is $\mathcal{F}_T$-measurable, then $Z \mathbbm{1}(T \leq t)$ is $\mathcal{F}_t$-measurable for all $t$ by definition.

	For the other way, if $Z = c \mathbbm{1}_{A}$, then $Z \mathbbm{1}(T \leq c)$ is $\mathcal{F}_t$-measurable for all $t$, then $Z$ is $\mathcal{F}_T$-measurable.

	For a general random variable $Z$, we can take finite linear combinations to prove this for simple functions, and then take limits.

	To prove that $X_T \mathbbm{1}(T < \infty)$ is $\mathcal{F}_T$-measurable, we will show that $X_T \mathbbm{1}(T \leq t)$ is $\mathcal{F}_t$-measurable for all $t$. We can write
	\[
	X_T \mathbbm{1}(T \leq t) = X_T \mathbbm{1}(T < t) + X_t \mathbbm{1}(T = t).
	\]
	Clearly $X_t \mathbbm{1}(T = t)$ is $\mathcal{F}_t$-measurable, so it suffices to show $X_T \mathbbm{1}(T < t)$ is $\mathcal{F}_t$-measurable.

	Let $T_n = 2^{-n} \lceil 2^{n} T \rceil$. Then $T_n$ is a stopping time with values in $\mathcal{D}_n = \{ k2^{-n} \mid k \in \mathbb{N}\}$. Indeed,
	\[
		\{ T_n \leq t\} = \{ \lceil 2^{n} T \rceil \leq 2^{n} t \} = \{ T \leq 2^{-n} \lfloor 2^{n} t \rfloor\} \in \mathcal{F}_{2^{-n} \lceil 2^{n} t \rceil} \subseteq \mathcal{F}_t.
	\]
	Now $T_n \downarrow T$ as $n \to \infty$. Since $X$ is cadlag,
	\[
	X_T = \mathbbm{1}(T < t) = \lim_{n \to \infty} X_{T_n \wedge t} \mathbbm{1}(T < t).
	\]
	Since $T_n$ takes on countably many values,
	\[
	X_{T_n \wedge t} \mathbbm{1}(T < t) = \sum_{\substack{d \in \mathcal{D}_n \\ d < t}} X_d \mathbbm{1}(T_n = d) + X_t \mathbbm{1}(T_n > t, T < t).
	\]
	Since $T_n$ is a stopping time, $X_{T_n \wedge t}\mathbbm{1}(T < t)$ is $\mathcal{F}_t$-measurable for all $n$.
\end{proofbox}

\begin{exbox}[Hitting times are not always stopping times]
	Let $J$ be a random variable with $\mathbb{P}(J = 1) = \mathbb{P}(J = -1) = 1/2$. Let
	\[
	X_t =
	\begin{cases}
		t & 0 \leq t \leq 1, \\
		1 + J(t - 1) & t > 1.
	\end{cases}
	\]
	Let $\mathcal{F}_t = \sigma(X_s \mid s \leq )$, the natural filtration, and let $A = (1, 2)$, with
	\[
		T_A = \inf \{t \geq 0 \mid X_t \in A\}.
	\]
	Then $\{T_A \leq 1\} \not \in \mathcal{F}_1$.
\end{exbox}

The fix is to impose regularity conditions on the process, or the filtration.

\begin{proposition}
	Let $A$ be closed and let $X$ be a continuous, adapted process. Then
	\[
		T_A= \inf \{t \geq 0 \mid X_t \in A\}
	\]
	is a stopping time.
\end{proposition}

\begin{proofbox}
	It suffices to show that
	\[
		\{T_A \leq t\} = \{ \inf_{\substack{s \in \mathbb{Q} \\ s \leq t}} d(X_s, A) = 0\},
	\]
	where $d(x, A)$ is the distance from $x$ to $A$. Then the latter element is $\mathcal{F}_t$-measurable.

	Suppose that $T_A = s \leq t$. Then there exists $(s_n)$ such that $X_{s_n} \in A$ for all $n$, and $s_ n\downarrow s$ as $n \to \infty$. Since $X$ is continuous, $X_{s_n} \to X_s$, and since $A$ is closed, $X_s = X_{T_A} \in A$.

	Then we can find $q_n \in \mathbb{Q}$ such that $q_n \uparrow T_A$. Since $d(X_{T_A}, A) = 0$, $d(X_{q_n}, A) \to 0$ as $n \to \infty$. This shows one inclusion.

	Now suppose that $\inf d(X_s, A) = 0$. Then there exists $s_n \in \mathbb{Q}$, with $s_n \leq t$ such that $d(X_{s_n}, A) \to 0$. By compactness, we can restrict to $s_n \to s$, and by continuity, $X_{s_n} \to X_s$, where $d(X_s, A) = 0$.

	Since $A$ is closed, this means $X_s \in A$, so $T_A \leq t$.
\end{proofbox}

\begin{definition}
	Let $(\mathcal{F}_t)$ be a filtration. Set
	\[
	\mathcal{F}_{t^+} = \bigcap_{s > tj \mathcal{F}_s}.
	\]
	If $\mathcal{F}_{t^+} = \mathcal{F}_t$ for all $t$, then $(\mathcal{F}_t)$ is \emph{right-continuous}\index{right-continuous filtration},
\end{definition}

% lecture 11

\begin{proposition}
	Let $A$ be an open set and $X$ a continuous process. Then
	\[
		T_A = \inf\{ t \geq 0 \mid X_t \in A\}
	\]
	is a stopping time with respect to $(\mathcal{F}_{t^+})$.
\end{proposition}

\begin{proofbox}
	Note that
	\[
		\{T_a < t\} = \bigcup_{\substack{q \in \mathbb{Q} \\ q< t}}\{X_q \in A\} \in \mathcal{F}_t,
	\]
	by the continuity of $X$ and the fact $A$ is open, and this being a countable union. Hence
	\[
		\{T_a \leq t\} = \bigcap_n \{T_A < t + 1/n\} \in \mathcal{F}_{t^{+}}.
	\]
\end{proofbox}

\subsection{Martingale Regularization Theorem}%
\label{sub:mrt}

A stochastic process is a random variable with values $\{f : \mathbb{R}_+ \to E\}$ with the $\sigma$-algebra which makes the evaluation maps $f \mapsto f(t)$ measurable for all $t \geq 0$.

The \emph{law}\index{law} of a stochastic process $X$ is the measure $\mu(A) = \mathbb{P}(X \in A)$, where $A$ is an event in this $\sigma$-algebra. In practice, this is not very easy to work with.

Suppose that $\mu$ is a probability measure on $D(\mathbb{R}_+, E)$. For each $J \subseteq \mathbb{R}_+$, let $\mu_J$ be the law $(X_t \mid t \in J)$. Then the family of probability measures $(\mu_J)$ are the finite dimensional distributions of $\mu$. Note that
\[
	\left\{ \bigcap_{s \in J} \{X_s \in \mathcal{A}_s\} \mid J \text{ finite}, \mathcal{A}_s \in \mathcal{B}(\mathbb{R})\right\}
\]
is a $\pi$-system, which generates the product $\sigma$-algebra on $D(\mathbb{R}_+, E)$. Hence Cad lag processes are define by their finite dimensional distributions.

\begin{definition}
	Let $X, X'$ be processes on the same probability space. Then $X'$ is a \emph{version}\index{version} of $X$ if $\mathbb{P}(X_t = X_t') = 1$ for all $t \geq 0$.
\end{definition}

\begin{exbox}
	Let $X = (X_t)_{t \in [0, 1]}$ be the process $X_t\equiv 0$. Let $U \sim U[0, 1]$, and $X_t' = \mathbbm{1}(U = t)$.

	Then the finite dimensional distributions of $X, X'$ are the same and $X'$ is a version of $X$. But $X'$ is not continuous, and
	\[
		\mathbb{P}(X_t' = 0 = X_t \text{ for all }t \in [0, 1]) = 0.
	\]
	In practice, it is useful to have a cadlag version of a process.
\end{exbox}

Let $(\Omega, \mathcal{F}, (\mathcal{F}_t), \mathbb{P})$ be a filtered probability space. Let $\mathcal{N} \subseteq \mathcal{F}$ be the collection of events with probability $0$, and define
\[
	\tilde{\mathcal{F}}_t = \sigma(\mathcal{F}_{t^+}, \mathcal{N}).
\]

\begin{definition}
	If $\mathcal{F}_t = \tilde{\mathcal{F}}_t$ for all $t \geq 0$, we say $(\mathcal{F}_t)$ satisfies the \emph{usual conditions}\index{usual conditions}.

	A continuous times process $X$ is a \emph{martingale}\index{martingale} if
	\[
		\mathbb{E}[X_t | \mathcal{F}_s] = X_s \qquad\text{a.s.}
	\]
	for all $0 \leq s \leq t$, and a submartingale or a supermartingale if the usual definitions hold.
\end{definition}

\begin{theorem}[Martingale Regularization]
	Let $X = (X_t)_{t \geq 0}$ be a martingale with respect to $(\mathcal{F}_t)$.

	Then there exists a cadlag process $\tilde X$ which is a martingale with respect to $(\tilde{\mathcal{F}}_t)$, and satisfies
	\[
		X_t = \mathbb{E}[\tilde X_t | \mathcal{F}_t] \qquad \text{a.s.}
	\]
	for all $t \geq 0$. If $(\mathcal{F}_t)$ satisfies the usual conditions, then $\tilde X$ is a cadlag version of $X$.
\end{theorem}

This is not unexpected; we saw that martingales tended to converge in the discrete time sense, both forwards and backwards. We can use this is idea in the continuous setting.

\begin{lemma}
	Let $f : \mathbb{Q}_+ \to \mathbb{R}$ be a function. Suppose that for all $a < b$, $a, b \in \mathbb{Q}$ and $I \subseteq \mathbb{Q}_+$ bounded, then $f$ is bounded on $I$, and define
	\begin{align*}
		N([a, b], I, f) &= \text{\normalfont{number of upcrossings that $f$ makes across $[a, b]$ in $I$}} \\
				&= \sup\{n \geq 0 \mid 0 \leq s_1 < t_2 < \cdots < s_n < t_n, s_i, t_i \in I, f(s_i) < a, f(t_i) > b\}.
	\end{align*}
	Suppose that $N([a, b], I, f) < \infty$ for all $a, b$ and $I$. Then for all $t \in \mathbb{R}_+$,
	\[
		\lim_{\substack{s \uparrow t \mid s \in \mathbb{Q}}} f(s), \qquad \lim_{\substack{s \downarrow t \mid s \in \mathbb{Q}}} f(s)
	\]
	exist and are finite.
\end{lemma}

\begin{proofbox}
	Suppose that $(s_n)$ is a sequence in $\mathbb{Q}_+$ which decreases to $t$. Then $\lim_n f(s_n)$ exists, since the number of upcrossings is finite.

	This limit does not depend on $(s_n)$, since if $(s_n')$ is another sequence, we can consider $(s_1, s_1', s_2, s_2', \ldots)$.

	We can use the same argument for $(s_n)$ in $\mathbb{Q}_+$ which increases to $t$.
\end{proofbox}

Now we are ready to prove martingale regularization.

\begin{proofbox}
	Our goal is to set
	\[
		\tilde X_t = \lim_{\substack{s \downarrow t \\ s \in \mathbb{Q}_+}} X_s
	\]
	on a set of probability 1, and show that the conclusions hold for $\tilde X_t$.

	The first step is to show that $X$ is bounded on bounded subsets of $\mathbb{Q}_+$. Fix $I \subseteq \mathbb{Q}_+$ bounded, and let $J = \{j_1, \ldots, j_n\} \subseteq I$, with $j_i$ in increasing order.

	Then $(X_j)$ is a discrete time martingale. From Doob's maximal inequality,
	\[
	\lambda \mathbb{P}(\max_{j \in J} |X_j| > \lambda) \leq \mathbb{E}[|X_{j_n}|]  \leq \mathbb{E}[|X_k|],
	\]
	where $k = \sup I < \infty$. Take a monotone limit over $J \subseteq I$ finite, which increases to $I$, to get
	\[
	\lambda \mathbb{P}(\sup_{t \in I} |X_t| > \lambda) \leq \mathbb{E}[|X_k|] < \infty.
	\]
	Here we use the fact that $I$ is a subset of $\mathbb{Q}_+$, which is countable. Hence $\mathbb{P}(\sup_{t \in I}|X_t| < \infty) = 1$.

	For step 2, fix $a < b$. We want to show that
	\[
		N([a, b], I, X) = \sup_{J \subseteq I \text{ finite}} N([a, b], J, X) < \infty.
	\]
	Fix $J = \{a_1, \ldots, a_n\}$ in increasing order. Then $(X_{a_i})$ is a martingale in discrete time, so we can apply Doob's upcrossing inequality, to get
	\[
		(b - a) \mathbb{E}[N([a, b], J, X)] \leq \mathbb{E}[(X_{a_n} - a)^{-}] \leq \mathbb{E}[(X_k - a)^{-}].
	\]
	From monotone convergence theorem, let $I_M = \mathbb{Q}_+ \cap [0, M]$, then
	\[
		N([a, b], I_M, X) < \infty.
	\]
	Now we start defining $\tilde X$. First of all we want to define the null set for which we do not care about $\tilde X$. Define
	\[
		\Omega_0 = \bigcap_{\substack{a < b \\ a, b \in \mathbb{Q} \\M \in \mathbb{N}}} (\{N([a, b], I_M, X) < \infty\} \cap \{\sup_{t \in I_M} |X_t| < \infty\}).
	\]
	Then from the above, this is a full set, i.e. $\mathbb{P}(\Omega_0) = 1$.

	From the lemma,
	\[
		X_{t^+} = \lim_{\substack{s \downarrow t \\ s \in \mathbb{Q}}} X_s, \qquad X_{t^{-}} = \lim_{\substack{s \uparrow t \\ s \in \mathbb{Q}}} X_s
	\]
	exist on $\Omega_0$. Set
	\[
	\tilde X_t = 
	\begin{cases}
		X_{t^+} &\text{on } \Omega_0, \\
		0 &\text{on } \Omega \setminus \Omega_0.
	\end{cases}
	\]
% lecture 12
	Then $(\tilde X_t)$ is $(\tilde{\mathcal{F}}_t)$-adapted, since $(\tilde{\mathcal{F}}_t)$ contains all $\mathbb{P}$-null sets for all $t \geq 0$.

	Our third step is to show that
	\[
		X_t = \mathbb{E}[\tilde X_t | \mathcal{F}_t] \qquad\text{a.s.}
	\]
	for all $t \geq 0$. Let $(t_n)$ be a sequence in $\mathbb{Q}$ with $t_n \downarrow t$ as $n \to \infty$. So
	\[
	\tilde X_t = \lim_{n \to \infty} X_{t_n}.
	\]
	The process $(X_{t_n})$ is a backwards martingale, hence converges almost surely and in $\mathcal{L}^1$, so
	\[
	\mathbb{E}[X_{t_n} | \mathcal{F}_t] \to \mathbb{E}[\tilde X_t | \mathcal{F}_t]
	\]
	in $\mathcal{L}^1$. But since $\mathbb{E}[X_{t_n} | \mathcal{F}_t] = X_t$ almost surely for all $n$, by the martingale property of $X$, we get
	\[
		X_t = \mathbb{E}[\tilde X_t | \mathcal{F}_t] \qquad \text{a.s.}
	\]
	Next (step four) we show that $(\tilde X_t)$ is an $(\tilde{\mathcal{F}}_t)$ martingale. Fix $s < t$, and let $(s_n)$ be a sequence in $\mathbb{Q}$ with $s_n \downarrow s$ and $s_0 < t$. Then,
	\[
	\tilde X_s = \lim_{n \to \infty} X_{s_n} = \lim_{n \to \infty} \mathbb{E}[X_t | \mathcal{F}_{s_n}].
	\]
	Since $(\mathbb{E}[X_t | \mathcal{F}_{s_n}]$ is a backwards martingale, it converges almost surely and in $\mathcal{L}^1$, to $\mathbb{E}[X_t | \mathcal{F}_{s^+}]$. Hence
	\[
	\tilde X_s = \mathbb{E}[X_t | \mathcal{F}_{s^+}].
	\]
	Two things to fix: first change $X_t$ to $\tilde X_t$, and $\mathcal{F}_{s^+}$ to $\tilde{\mathcal{F}}_s$. For the first, note for any $T > t > s$,
	\begin{align*}
		\tilde X_s &= \mathbb{E}[X_T | \mathcal{F}_{s^{+}}] = \mathbb{E}[\mathbb{E}[X_T | \mathcal{F}_{t^+}] | \mathcal{F}_{s^+}] \\
			   &= \mathbb{E}[\tilde X_t | \mathcal{F}_{s^+}].
	\end{align*}
	Note that if $\mathcal{G}$ is a $\sigma$-algebra, and $X \in \mathcal{L}^1$, then
	\[
		\mathbb{E}[X |\sigma(\mathcal{G}, \mathcal{N})] = \mathbb{E}[X | \mathcal{G}]\qquad \text{a.s.}
	\]
	Applying this gives
	\[
		\tilde X_s = \mathbb{E}[\tilde X_t | \tilde{\mathcal{F}}_s]\qquad \text{a.s.}
	\]
	Finally, for step five we show that $\tilde X_t$ has the cadlag property.

	Suppose that $\omega \in \Omega_0$ is such that $\tilde X(\omega)$ is not right-continuous. Then there exists $t$ and $(s_n)$ with $s_n \downarrow t$ as $n \to \infty$ so that
	\[
	|\tilde X_{s_n}(\omega) - \tilde X_t(\omega)| > \eps
	\]
	for some $\eps > 0$, and all $n$. By the definition of $\tilde X$, there exists $(s_n') \in \mathbb{Q}$ with $s_n' > s_n$ and $s_n' \downarrow t$, with
	\[
	|\tilde X_{s_n}(\omega) - X_{s_n'}(\omega)| \leq \eps/2,
	\]
	which gives
	\[
	|X_{s_n'}(\omega) - \tilde X_t(\omega)| \geq \eps/2.
	\]
	This contradicts $X_{s_n'}(\omega) = \tilde X_t(\omega)$, so $\tilde X$ is right-continuous on $\Omega_0$.

	To show $\tilde X$ has left limits is an exercise.
\end{proofbox}

\begin{exbox}
	Let $\xi, \eta$ be independent random variables which are $\pm1$ with probability $1/2$. Set
	\[
	X_t = 
	\begin{cases}
		0 & t < 1, \\
		\xi & t = 1, \\
		\xi + \eta & t > 1.
	\end{cases}
	\]
	Let $\mathcal{F}_t = \sigma(X_s \mid s \leq t)$. We can see that $(X_t)$ is a $(\mathcal{F}_t)$-martingale, but this is not right-continuous at $t = 1$.

	It is also easy to see that $\mathcal{F}_1 = \sigma(\xi)$, but $\mathcal{F}_{1^+} = \sigma(\xi, \eta)$. To make a cadlag process, we set
	\[
	\tilde X_t =
	\begin{cases}
		0 & t < 1,\\
		\xi + \eta & t \geq 1.
	\end{cases}
	\]
	Then $X_t = \mathbb{E}[\tilde X_t | \mathcal{F}_t]$ almost-surely for all $t \geq 0$. Moreover $\tilde X$ is a martingale with respect to $(\mathcal{F}_{t^+})$, and is cadlag.

	But $\tilde X$ is not a version of $X$, since $X_1 \neq \tilde X_1$ almost-surely.
\end{exbox}

From now n, we always consider the cadlag version of a martingale when the filtration satisfies the usual conditions.

\subsection{Convergence and Doob's Inequalities}%
\label{sub:con_doob}

\begin{theorem}[Martingale Convergence]
	Let $(X_t)$ be a $\mathcal{L}^1$ bounded cadlag martingale. Then $X_t \to X_\infty$ almost-surely for $X_\infty \in \mathcal{L}^1(\mathcal{F}_\infty)$, where $\mathcal{F}_\infty = \sigma(\mathcal{F}_t \mid t \geq 0)$.
\end{theorem}

\begin{proofbox}
	Let $N([a, b], I_m, X)$ be the number of upcrossings across $[a, b]$ in the interval $I_m = [0, m] \cap \mathbb{Q}$.

	As in martingale regularization,
	\[
		(b - a) \mathbb{E}[N([a, b], I_m, X)] \leq |a| + \sup_{t \geq 0} \mathbb{E}[|X_t|] < \infty,
	\]
	since $X$ is $\mathcal{L}^1$-bounded. From MCT,
	\[
		(b - a) \mathbb{E}[N([a, b], \mathbb{Q}_+, X)] \leq |a| + \sup_{t \geq 0} \mathbb{E}[|X_t|] < \infty,
	\]
	hence $N([a, b], \mathbb{Q}, X) < \infty$ almost-surely. Again let
	\[
		\Omega_0 = \bigcap_{\substack{a < b \\ a, b\in \mathbb{Q}}} \{N([a, b], \mathbb{Q}_+, X) < \infty\}.
	\]
	Then $\mathbb{P}(\Omega_0) = 1$. On $\Omega_0$, $X_q \to X_\infty$ as $q \to \infty$, for $q \in \mathbb{Q}_+$. We now use the cadlag property to show that $X_t \to X_\infty$ as $t \to \infty$. Since $X_q \to X_\infty$, for all $\eps > 0$ there is $q_0$ with
	\[
	|X_q - X_\infty| \leq \eps/2,
	\]
	for all $q \geq q_0$. From right-continuity, if $t > q_0$, then there is $q \in \mathbb{Q}_+$ with, for all $q > t$, $|X_t - X_q| < \eps / 2$. Combining these inequalities gives
	\[
	|X_t - X_\infty| < \eps
	\]
	for all $t > q_0$, showing $X_t \to X_\infty$.
\end{proofbox}

\begin{theorem}[Doob's Maximal Inequality]
	Let $(X_t)$ be a cadlag martingale, and define
	\[
	X_t^{\ast} = \sup_{0 \leq s \leq t} |X_s|.
	\]
	Then for all $\lambda \geq 0$ and $t \geq 0$,
	\[
	\lambda \mathbb{P}(X_t^{\ast} \geq \lambda) \leq \mathbb{E}[|X_t| \mathbbm{1}(X_t^{\ast} \geq \lambda)] \leq \mathbb{E}[|X_t|].
	\]
\end{theorem}

\begin{proofbox}
	From the cadlag property,
	\[
		\sup_{s \leq t} |X_s| = \sup_{s \in \{t\} \cup([0, t) \cap \mathbb{Q})} |X_s|.
	\]
	Then we can argue as before.
\end{proofbox}

We will list the following statements which we have proven for discrete time martingales and can be easily extended to continuous time.

\begin{theorem}[Doob's $\mathcal{L}^p$ Inequality]Let $(X_t)$ be a cadlag martingale, and $p > 1$. Let $X_t^{\ast} = \sup_{s \leq t} |X_s|$. Then
	\[
	\|X_t^{\ast}\|_p \leq \frac{p}{p-1} \|X_t\|_p.
	\]
\end{theorem}

\begin{theorem}
	Let $(X_t)$ be a cadlag martingale, and $p > 1$. Then then following are equivalent:
	\begin{enumerate}[\normalfont(i)]
		\item $X$ is $\mathcal{L}^p$ bounded.
		\item $X$ converges almost-surely and in $\mathcal{L}^1$ to $X_\infty$.
		\item There exists $Z \in \mathcal{L}^p$ with $X_t = \mathbb{E}[Z | \mathcal{F}_t]$ for all $t \geq 0$.
	\end{enumerate}
\end{theorem}

\begin{theorem}
	Let $(X_t)$ be a cadlag martingale. Then the following are equivalent:
	\begin{enumerate}[\normalfont(i)]
		\item $X$ is uniformly integrable.
		\item $X$ converges almost-surely and in $\mathcal{L}^1$ to $X_\infty$.
		\item There exists $Z \in \mathcal{L}^1$ with $X_t = \mathbb{E}[Z | \mathcal{F}_t]$ almost-surely, for all $t \geq 0$.
	\end{enumerate}
\end{theorem}

% lecture 13

\begin{theorem}[Optional Stopping Theorem]
	Let $X$ be a cadlag, UI martingale. Then for all $S \leq T$ stopping times,
	\[
		\mathbb{E}[X_T | \mathcal{F}_S] = X_S \qquad\text{a.s.}
	\]
\end{theorem}

\begin{proofbox}
	Let $A \in \mathcal{F}_S$, then we need to show that
	\[
	\mathbb{E}[X_T \mathbbm{1}_{A}] = \mathbb{E}[X_S \mathbbm{1}_{A}].
	\]
	Let $T_n = 2^{-n} \lceil 2^{n} T \rceil$, and $S_n = 2^{-n} \lceil 2^{n} S \rceil$. Then $S_n \downarrow S$, $T_n \downarrow T$ as $n \to \infty$. By right continuity, $X_{S_n} \to X_S$, $X_{T_n} \to X_T$ as $n \to \infty$.

	From optional stopping theorem in the discrete case, we have $X_{T_n} = \mathbb{E}[X_\infty | \mathcal{F}_{T_n}]$, hence $X_{T_n}$ is UI (as a conditional expectation of the same variable), so convergence $X_{T_n} \to X_T$ happens in $\mathcal{L}^1$. Similarly $X_{S_n} \to X_S$ in $\mathcal{L}^1$.

	From optional stopping theorem in the discrete sense, $X_{S_n} = \mathbb{E}[X_{T_n}] | \mathcal{F}_{S_n}]$. Since $A \in \mathcal{F}_S \subseteq \mathcal{F}_{S_n}$ as $S_n \geq S$, we have
	\[
	\mathbb{E}[X_{T_n} \mathbbm{1}_{A}] = \mathbb{E}[X_{S_n} \mathbbm{1}_{A}].
	\]
	Take $n \to \infty$. From $\mathcal{L}^1$ convergence,
	\[
	\mathbb{E}[X_T \mathbbm{1}_{A}] = \mathbb{E}[X_S \mathbbm{1}_{A}].
	\]
\end{proofbox}

\subsection{Kolmogorov Continuity Criterion}%
\label{sub:kcc}

Let $\mathcal{D}_n = \{k 2^{-n} \mid 0 \leq k \leq 2^{n}\}$, and $\mathcal{D} = \bigcup \mathcal{D}_n$.

\begin{theorem}
	Let $(X_t)_{t \in \mathcal{D}}$ be a $\mathbb{R}$-valued process. Suppose that there exists $p, s > 0$ so that
	\[
		\mathbb{E}[|X_s - X_t|^{p}]] \leq C |t - s|^{1 + \eps},
	\]
	for all $s, t \in \mathcal{D}$ for some $C < \infty$. Then for $\alpha \in (0, \eps/p)$, the process $(X_t)_{t \in \mathcal{D}}$ is $\alpha$-H\"older continuous, i.e. there exists $K_\alpha <  \infty$ such that
	\[
	|X_t - X_s| \leq K_\alpha |t - s|^{\alpha},
	\]
	for all $s, t \in \mathcal{D}$.
\end{theorem}

\begin{proofbox}
	From Markov's inequality,
	\[
		\mathbb{P}(|X_{k2^{-n}} - X_{(k+1)2^{-n}}| \geq 2^{-n \alpha} ) \leq C 2^{n \alpha p}\cdot 2^{-n(1 + \eps)},
	\]
	from the given condition. The union bound gives
	\[
		\mathbb{P}\left( \max_{1 \leq k \leq 2^{n}} |X_{k2^{-n}} - X_{(k+1)2^{-n}}| \geq 2^{-n \alpha}\right) \leq C 2^{-n(\eps - \alpha p)}.
	\]
	Since $\alpha \in (0, \eps/p)$, from Borel-Cantelli, since the RHS is summable,
	\[
	\max_{0 \leq k \leq 2^{n}} |X_{k2^{-n}} - X_{(k+1)2^{-n}}| \leq 2^{-n \alpha},
	\]
	for all $n$ sufficiently large. Hence there exists $M$ such that
	\[
	\sup_{n \geq 0} \max_{0 \leq k \leq 2^{n}} \frac{|X_{k2^{-n}} - X_{(k+1)2^{-n}}|}{2^{-n\alpha}} \leq M < \infty.
	\]
	Now we need to transform this $M$ into $M'$ such that
	\[
	|X_t - X_s| \leq M' |t - s|^{\alpha},
	\]
	for all $t, s \in \mathcal{D}$. Fix $s, t \in \mathcal{D}$, and fix $r \in \mathbb{Z}$ such that $2^{-r-1} < t - s \leq 2^{-r}$. So there exists $k$ such that
	\[
	s < k2^{-(r+1)} < t.
	\]
	Let $\beta$ be this dividing dyadic. We know that $0 < t - \beta < 2^{-r}$, so write
	\[
	t - \beta = \sum_{j = r+1}^\infty \frac{x_j}{2^{j}}
	\]
	for $x_j \in \{0, 1\}$. Similarly we can write
	\[
	\beta - s = \sum_{j = r + 1}^{\infty} \frac{y_j}{2^{j}},
	\]
	for $y_j \in \{0, 1\}$. This partitions $[s, t)$ into a disjoint union of dyadic intervals of length $2^{-n}$ where $n \geq r+1$. Importantly, there are at most $2$ intervals of each length. Hence
	\[
	|X_t - X_s| \leq 2 \sum_{n \geq r + 1} M 2^{-n \alpha} = 2 M \frac{2^{-(r+1)\alpha}}{1 - 2^{-\alpha}}.
	\]
	Set $M' = 2M / (1 - 2^{-\alpha})$. THen we get
	\[
	|X_t - X_s| \leq M' 2^{-(r+1)\alpha} \leq M' |t - s|^{\alpha}.
	\]
\end{proofbox}

\newpage

\section{Weak Convergence}%
\label{sec:wc}

Let $(M, d)$ be a metric space equipped with the Borel $\sigma$-algebra.

\begin{definition}
	Let $(\mu_n)$ be a sequence of probability measures on $M$. We say that $\mu_n \to \mu$ \emph{weakly}\index{weak convergence} if $\mu_n(f) \to \mu(f)$ for all $f$ bounded, continuous.
\end{definition}

\begin{exbox}
	\begin{enumerate}
		\item Let $(x_n)$ be the sequence in $M$ with $x_n \to x$ as $n \to \infty$. Then $\delta_{x_n} \to \delta_x$ as $n \to \infty$. Indeed,
			\[
			\delta_{x_n}(f) = f(x_n) \to f(x) = \delta_x(f)
			\]
			 for all $f$ continuous.
		 \item Let $M = [0, 1]$ with the Euclidean metric, and let
			 \[
			 \mu_n = \frac{1}{n} \sum_{k = 0}^{n-1} \delta_{k/n}.
			 \]
			 Then we have
			 \[
			 \mu_n(f) = \frac{1}{n} \sum_{k = 0}^{n-1} f(k/n).
			 \]
			 This is a Riemann sum, which converges to
			 \[
			 \int_0^1 f(x) \diff x
			 \]
			 for $f$ continuous. Hence $\mu_n \implies \mu$, the Lebesgue measure on $[0, 1]$.
	\end{enumerate}
\end{exbox}

\begin{remark}
	If $A$ is a Borel set, it is not true that $\mu_n \implies \mu$ implies that $\mu_n(A) = \mu(A)$ as $n \to \infty$.

	For example, let $x_n = \frac{1}{n}$, and $\mu_n = \delta_{x_n}$, then $\mu_n \implies \delta_0.$ 

	Take $A = (0, 1)$, then $\mu_n(A) = 1$ for all $n \geq 2$, but $\delta_0(A) = 0$.
\end{remark}

\begin{theorem}
	Suppose that $\mu_n$ is a sequence of probability measures. The following are equivalent:
	\begin{enumerate}[\normalfont(i)]
		\item $\mu_n \implies \mu$.
		\item $\liminf \mu_n(G) \geq \mu(G)$ for all $G$ open.
		\item $\limsup \mu_n(A) \leq \mu(A)$ for all $A$ closed.
		\item $\lim \mu_n(A) = \mu(A)$ for all $A$ with $\mu(\partial A) = 0$.
	\end{enumerate}
\end{theorem}

\begin{proofbox}
	

	(i) $\implies$ (ii): Let $G$ be open with $G^{c} \neq \emptyset$. For each $M > 0$, set
	\[
	f_M(x) = 1 \wedge(M d(x, G^{c})).
	\]
	Then $f_M$ is bounded, continuous and $f_M \uparrow \mathbbm{1}_{G}$ as $M \to \infty$. Maybe we can use Tietze in the non-metric case. Hence
	\[
	\mu_n(G) \geq \mu_n(f_M) \to \mu(f_M)
	\]
	as $n \to \infty$. Then
	\[
	\liminf_{n \to \infty} \mu_n(G) \geq \liminf_{n \to \infty} \mu_n(f_M) = \mu(f_M).
	\]
	Now take $f_M \uparrow \mathbbm{1}_{G}$ and MCT to find
	\[
	\liminf_{n \to \infty} \mu_n(G) \geq \mu(G).
	\]

	(ii) $\implies$ (iii): This is easy to see as the complement of an open set is closed. Similarly (iii) $\implies$ (ii).
% lecture 14
\end{proofbox}

% lecture 15

\newpage

\printindex

\end{document}
